{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216b26ae",
   "metadata": {},
   "source": [
    "## 1.Import 및 라이브러리 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa840e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "sys.path.append('~/aiffel/Aiffelthon_koBART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6675e2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.21.4)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: datasets==1.0.2 in /opt/conda/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.21.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (4.62.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (0.3.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.26.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers==4.24.0 in /opt/conda/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.13.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (3.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (1.21.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2021.10.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformer-utils in /opt/conda/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (1.9.1+cu111)\n",
      "Requirement already satisfied: colorcet in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (3.0.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.24.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.62.3)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (0.11.2)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /opt/conda/lib/python3.9/site-packages (from colorcet->transformer-utils) (0.4.8)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.21.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (3.4.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->transformer-utils) (4.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.13.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (3.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (3.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (8.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->seaborn->transformer-utils) (2021.3)\n",
      "Requirement already satisfied: param>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from pyct>=0.4.4->colorcet->transformer-utils) (1.12.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (1.26.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn->transformer-utils) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "!pip install datasets==1.0.2\n",
    "!pip install transformers==4.24.0\n",
    "!pip install transformer-utils\n",
    "!pip install packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3aba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "#Tokenizer\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "#Encoder-Decoder Model\n",
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "#Training\n",
    "from seq2seq_trainer import Seq2SeqTrainer\n",
    "from transformers import TrainingArguments\n",
    "from seq2seq_training_args import Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d36ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 20%sen :  73431\n",
      "val 20%sen :  73431\n"
     ]
    }
   ],
   "source": [
    "# 1. Data EDA\n",
    "### 이 과정에서는 Data의 특성을 파악하고 얼만큼의 Data를 사용할 것인지 파악할 것이다\n",
    "#- 예를 들어 토큰화 이후 Data별 Len을 확인하였을 때 적절한 len을 찾는 것이 목표이다.\n",
    "\n",
    "\n",
    "# 데이터 불러오기\n",
    "train_20sent = pd.read_csv('data/train_20per_Sum3.csv')\n",
    "val_20sent = pd.read_csv('data/train_20per_Sum3.csv')\n",
    "\n",
    "\n",
    "# 데이터 별 길이\n",
    "# 현재 train, val의 길이가 동일한 것으로 나오는데 데이터 추출에서 문제가 발생한 것으로 예측됨.\n",
    "\n",
    "print('train 20%sen : ',len(train_20sent))\n",
    "print('val 20%sen : ',len(val_20sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "231304e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_20sent['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ef4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.str.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 제거\n",
    "    sentence = re.sub(\"'\",'', sentence) # 따옴표 제거\n",
    "    sentence = re.sub('\\n','', sentence) # \\n \" 제거\n",
    "    sentence = re.sub('.{2,3}\\W{0,1}기자','', sentence) # 기자 이름 제거\n",
    "    sentence = re.sub(r'[?.!,][/?.!,]', '', sentence) # 여러개 문장 부호를 하나의 문장부호로 바꿉니다\n",
    "    sentence = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-z0-9]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d15e885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73431/73431 [00:00<00:00, 1874775.92it/s]\n",
      "100%|██████████| 73431/73431 [00:00<00:00, 2100699.37it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "clean_headlines = []\n",
    "\n",
    "for i in tqdm(train_20sent['input']):\n",
    "    clean_text.append(i)\n",
    "for i in tqdm(train_20sent['sentence_per_20']):\n",
    "    clean_headlines.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c7842e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_20sent['input'] = clean_text\n",
    "train_20sent['sentence_per_20'] = clean_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c217f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_index 사용\n",
    "train_20sent.reset_index(inplace=True, drop=True)\n",
    "val_20sent.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7efc2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_20sent) \n",
    "val_len = len(val_20sent) // 2\n",
    "val_data = Dataset.from_pandas(val_20sent[:val_len])\n",
    "test_data=Dataset.from_pandas(val_20sent[val_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2ffd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'input': Value(dtype='string', id=None), 'sentence_per_20': Value(dtype='string', id=None)}, num_rows: 73431)\n",
      "Dataset(features: {'input': Value(dtype='string', id=None), 'sentence_per_20': Value(dtype='string', id=None)}, num_rows: 36715)\n",
      "Dataset(features: {'input': Value(dtype='string', id=None), 'sentence_per_20': Value(dtype='string', id=None)}, num_rows: 36716)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2841349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 512\n",
    "max_target = 128\n",
    "batch_size = 3\n",
    "model_checkpoints = \"gogamza/kobart-base-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fc03ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a59627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_length = 256  # demo\n",
    "decoder_max_length = 64 # 글자가 끊김 -> 이걸 더 늘려줄 필요가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b10634ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "  #get all the dialogues\n",
    "  inputs = [dialogue for dialogue in data_to_process['input']]\n",
    "  #tokenize the dialogues\n",
    "  model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
    "  #tokenize the summaries\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(data_to_process['sentence_per_20'], max_length=max_target, padding='max_length', truncation=True)\n",
    "    \n",
    "  #set labels\n",
    "  model_inputs['labels'] = targets['input_ids']\n",
    "  #return the tokenized data\n",
    "  #input_ids, attention_mask and labels\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b404a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e604348a1c194f39a624012f52bf30df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fcd87a9bc34ce886553d54ed17e0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['input', 'sentence_per_20'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['input', 'sentence_per_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f120cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fa1a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53d5a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results6\",\n",
    "    num_train_epochs=5,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=2000,\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1158380",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "062fe7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f893fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 73431\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 22950\n",
      "  Number of trainable parameters = 123859968\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22950' max='22950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22950/22950 7:55:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.450600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.441100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.421800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.406400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results6/checkpoint-500\n",
      "Configuration saved in results6/checkpoint-500/config.json\n",
      "Model weights saved in results6/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to results6/checkpoint-1000\n",
      "Configuration saved in results6/checkpoint-1000/config.json\n",
      "Model weights saved in results6/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to results6/checkpoint-1500\n",
      "Configuration saved in results6/checkpoint-1500/config.json\n",
      "Model weights saved in results6/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to results6/checkpoint-2000\n",
      "Configuration saved in results6/checkpoint-2000/config.json\n",
      "Model weights saved in results6/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-2500\n",
      "Configuration saved in results6/checkpoint-2500/config.json\n",
      "Model weights saved in results6/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-3000\n",
      "Configuration saved in results6/checkpoint-3000/config.json\n",
      "Model weights saved in results6/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-3500\n",
      "Configuration saved in results6/checkpoint-3500/config.json\n",
      "Model weights saved in results6/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-4000\n",
      "Configuration saved in results6/checkpoint-4000/config.json\n",
      "Model weights saved in results6/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-4500\n",
      "Configuration saved in results6/checkpoint-4500/config.json\n",
      "Model weights saved in results6/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-5000\n",
      "Configuration saved in results6/checkpoint-5000/config.json\n",
      "Model weights saved in results6/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-5500\n",
      "Configuration saved in results6/checkpoint-5500/config.json\n",
      "Model weights saved in results6/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-6000\n",
      "Configuration saved in results6/checkpoint-6000/config.json\n",
      "Model weights saved in results6/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-6500\n",
      "Configuration saved in results6/checkpoint-6500/config.json\n",
      "Model weights saved in results6/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-7000\n",
      "Configuration saved in results6/checkpoint-7000/config.json\n",
      "Model weights saved in results6/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-7500\n",
      "Configuration saved in results6/checkpoint-7500/config.json\n",
      "Model weights saved in results6/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-8000\n",
      "Configuration saved in results6/checkpoint-8000/config.json\n",
      "Model weights saved in results6/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-8500\n",
      "Configuration saved in results6/checkpoint-8500/config.json\n",
      "Model weights saved in results6/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-9000\n",
      "Configuration saved in results6/checkpoint-9000/config.json\n",
      "Model weights saved in results6/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-9500\n",
      "Configuration saved in results6/checkpoint-9500/config.json\n",
      "Model weights saved in results6/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-10000\n",
      "Configuration saved in results6/checkpoint-10000/config.json\n",
      "Model weights saved in results6/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-10500\n",
      "Configuration saved in results6/checkpoint-10500/config.json\n",
      "Model weights saved in results6/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-11000\n",
      "Configuration saved in results6/checkpoint-11000/config.json\n",
      "Model weights saved in results6/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-11500\n",
      "Configuration saved in results6/checkpoint-11500/config.json\n",
      "Model weights saved in results6/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-12000\n",
      "Configuration saved in results6/checkpoint-12000/config.json\n",
      "Model weights saved in results6/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-12500\n",
      "Configuration saved in results6/checkpoint-12500/config.json\n",
      "Model weights saved in results6/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-13000\n",
      "Configuration saved in results6/checkpoint-13000/config.json\n",
      "Model weights saved in results6/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-13500\n",
      "Configuration saved in results6/checkpoint-13500/config.json\n",
      "Model weights saved in results6/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-14000\n",
      "Configuration saved in results6/checkpoint-14000/config.json\n",
      "Model weights saved in results6/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-14500\n",
      "Configuration saved in results6/checkpoint-14500/config.json\n",
      "Model weights saved in results6/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-15000\n",
      "Configuration saved in results6/checkpoint-15000/config.json\n",
      "Model weights saved in results6/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-15500\n",
      "Configuration saved in results6/checkpoint-15500/config.json\n",
      "Model weights saved in results6/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-16000\n",
      "Configuration saved in results6/checkpoint-16000/config.json\n",
      "Model weights saved in results6/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-16500\n",
      "Configuration saved in results6/checkpoint-16500/config.json\n",
      "Model weights saved in results6/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-17000\n",
      "Configuration saved in results6/checkpoint-17000/config.json\n",
      "Model weights saved in results6/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-17500\n",
      "Configuration saved in results6/checkpoint-17500/config.json\n",
      "Model weights saved in results6/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-18000\n",
      "Configuration saved in results6/checkpoint-18000/config.json\n",
      "Model weights saved in results6/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-18500\n",
      "Configuration saved in results6/checkpoint-18500/config.json\n",
      "Model weights saved in results6/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-19000\n",
      "Configuration saved in results6/checkpoint-19000/config.json\n",
      "Model weights saved in results6/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-19500\n",
      "Configuration saved in results6/checkpoint-19500/config.json\n",
      "Model weights saved in results6/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-20000\n",
      "Configuration saved in results6/checkpoint-20000/config.json\n",
      "Model weights saved in results6/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-20500\n",
      "Configuration saved in results6/checkpoint-20500/config.json\n",
      "Model weights saved in results6/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-21000\n",
      "Configuration saved in results6/checkpoint-21000/config.json\n",
      "Model weights saved in results6/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-21500\n",
      "Configuration saved in results6/checkpoint-21500/config.json\n",
      "Model weights saved in results6/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-22000\n",
      "Configuration saved in results6/checkpoint-22000/config.json\n",
      "Model weights saved in results6/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to results6/checkpoint-22500\n",
      "Configuration saved in results6/checkpoint-22500/config.json\n",
      "Model weights saved in results6/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in results6/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in results6/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [results6/checkpoint-21000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22950, training_loss=1.475346052049292, metrics={'train_runtime': 28541.6505, 'train_samples_per_second': 12.864, 'train_steps_per_second': 0.804, 'total_flos': 1.119338946625536e+17, 'train_loss': 1.475346052049292, 'epoch': 5.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c1ca677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 36715\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2295' max='2295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2295/2295 34:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.4190137386322021,\n",
       " 'eval_rouge2_precision': 0.1506,\n",
       " 'eval_rouge2_recall': 0.0689,\n",
       " 'eval_rouge2_fmeasure': 0.0866,\n",
       " 'eval_runtime': 2095.0949,\n",
       " 'eval_samples_per_second': 17.524,\n",
       " 'eval_steps_per_second': 1.095,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eea2bb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v1/snapshots/d7e64abd841bc1fa5d2939d14161124c51f29e8b/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v1\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 1.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v1/snapshots/d7e64abd841bc1fa5d2939d14161124c51f29e8b/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at gogamza/kobart-base-v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "통과2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  scores = () if (return_dict_in_generate and output_scores) else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "통과2\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"input\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    print('통과2')\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(\"gogamza/kobart-base-v1\")# 여기에 기본 kobart가져오기?\n",
    "\n",
    "test_samples = val_data.select(range(16))\n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1] # 여기에 체크포인트 가져오기 \n",
    "# 연구해봐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0a0a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e592211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Id  Summary after                                                                                    Summary before\n",
      "----  -----------------------------------------------------------------------------------------------  ------------------------------------------------------------------------\n",
      "   0  독일계 음식 배달서비스업체 DH(딜리버리 히어로)가 평가한 우아                                     40억 달러 ‘ ‘ ‘’ 주인공 김봉진 우아한형제들 대표태\n",
      "   1  한·중 수교 이후 지금까지의 한·중관계는 경제적 협력 관계를 중심으로 발전해 왔다. 이로             예상  베이징  베이징에서 열린 한·중 정상회담을 계기로 내년 봄 시진핑\n",
      "   2  한·중이 사드에 관한 ‘3불(不)’이 ‘약속’인지 ‘                                                     속’ ‘약속’인지 ‘입장표명’인지 표현을 놓고 갈등하는\n",
      "   3  배달의민족이 독일 자본에 매각된 것을 놓고 말들이 많다. 민족 정서를                               배달의민족이 독일 자본에 매각된 것을 놓고 말들이 사람들이  민족 정서를\n",
      "   4  지난 28일부터 나흘간 진행된 7기 5차 노동당 전원회의에서 북한은 핵 무력 개발의                    북한 전원회의에서 북한은 핵        북한 전원회의에서 북한은 핵\n",
      "   5  부산 해운대의 상징이었던 5성급 해운대그랜드호텔(그랜드호텔)이 지난해                             은 부산대의 상징이었던 5성급 해운대그랜드호텔(그랜드호텔\n",
      "   6  워런은 지난해 하반기 최고 관심 후보였다. 지난해 10월 아이오와주와 뉴햄프                         지지율 3위: 엘리자베스 워런 ◈강점◈  지지율 3위\n",
      "   7  1월 1일 새해 김정은 북한 국무위원장은 모습을 드러내지 않았다. 2012년 집권 이후 처음이다. 신년사  1월  북한    신년사 육성 연설은 대외 메시지인 동시에 북한 주민을\n",
      "   8  국내 이동통신 3사(KT·SK텔레콤·LG유플러스)는 산토끼(                                              거토         잡토끼(신규 고객)를\n",
      "   9  카를로스 곤(66)닛산자동차 전 회장의 ‘보석기간 중 탈출극\n",
      "  10  31일(현지시간) 미국 ABC의 신년 전야 특집 프로그램인 ‘딕클락                                      방탄소년단(BTS)이 미국 뉴욕에서 2020년 새해를라디오  31일(현지\n",
      "  11  매일 커피전문점을 찾는 이지은(32) 씨는 결제할 때마다 5초쯤 멈춘                                  매일 커피전문점을 찾는 이지은(32) 씨는 결제할 때마다 5초쯤 멈춘\n",
      "  12  '우주 굴기(崛起)'를 내세우며 세계 최초로 달의 뒷면에 탐사                                        지난해 지난해 9월 일본을 방문한 짐 브라이든스틴 미 항공우주국(NAS\n",
      "  13  올해 4월 치러지는 총선 예비후보로 지난 12월 말 기준 여야 현역 의원 8명이 등록을                  올해 말  기준 여야 현역 의원 8명이 등록을 마친 것으로 입력  통상 정치 신\n",
      "  14  올해 수출을 비롯한 경기가 지난해보다 나아질 거라는 정부의 전망은 반도체 업황 회복 가능성에       근거적원 견해 견해 견해 나오지 ” ” ” ” ” ”(홍남기 부\n",
      "  15  황 대표가 1일 서울 여의도에서 가진 기자간담회에서 “하나 된 힘으로 저들(정부·여\n",
      "\n",
      "Target summaries:\n",
      "\n",
      "  Id  Target summary\n",
      "----  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "   0  독일계 음식 배달서비스업체 DH(딜리버리 히어로)가 평가한 우아한형제들의 기업가치는 약 4조8000억원. 두 회사가 절반씩 출자해 싱가포르에 세우는 합작법인 ‘우아DH아시아’의 책임자로서 아시아 11개국 사업을 총괄하게 된다. 전 직원들에게 연말 특별 휴가를 선물한 김봉진(44) 대표는 혼자 출근해 남은 일을 처리하고 있었다. 국내 스타트업 M&A 사상 최대 규모다.\n",
      "   1  한·중관계가 평등하기보다는 중국 쪽으로 ‘운동장이 기울어진’ 비대칭관계로 서서히 바뀐 것은 격세지감을 느낄 정도로 중국의 위상이 높아짐에 따른 양자관계 변화가 저변에 깔려있다. 더불어 한미동맹 문제 등 근래 한국과 주요 우방과의 관계 악화, 그리고 세계무대에서 한국의 경제 지위 하강은 중국이 생각하는 한국의 몸값을 더욱 낮게 보이게 할 수 있다.\n",
      "   2  한·중이 사드에 관한 ‘3불(不)’이 ‘약속’인지 ‘입장표명’인지 표현을 놓고 갈등하는 모양새도 해석의 차이가 있지만, 결국은 중국이 한국을 압박할 수 있는 카드다. 예를 들어 미국이 중거리 미사일을 한국에 배치하면 중국은 ‘3불 위반’으로 몰아갈 수 있다. 한·중관계는 다시 악화될 것이고, 그 파동은 분명 사드 때보다 더 큰 양자 관계 파열이 될 것이다.\n",
      "   3  배달의민족이 독일 자본에 매각된 것을 놓고 말들이 많다. 민족 정서를 배반했다며 ‘배다른 민족’ ‘게르만 민족’이라 부르자는 분노가 터져 나온다. 하지만 창업 9년 만에 매출액 3193억원, 영업이익 586억원의 벤처가 무려 40억 달러(약 4조7500억원)에 매각된 것은 디지털 혁명 시대를 알리는 가장 분명한 신호다.\n",
      "   4  지난 28일부터 나흘간 진행된 7기 5차 노동당 전원회의에서 북한은 핵 무력 개발의 주역인 이병철 군수공업부 제1부부장을 정치국 위원과 당 부위원장, 부장(군수공업부)에 임명했다. 정치국 위원은 평상시 노동당의 정책을 결정하는 정치국 회의에서 투표권을 행사하는 자리로, 노동당의 핵심 인사로 꼽힌다.\n",
      "   5  부산 해운대의 상징이었던 5성급 해운대그랜드호텔(그랜드호텔)이 지난해 12월 31일 폐업했다. 그랜드호텔측은 호텔고급화 경쟁에 밀려 적자 경영으로 폐업이 불가피하다고 한다. 반면 그랜드호텔 노조는 고용 승계를 피하려는 위장 폐업이라고 주장한다. 그랜드호텔측은 1일 “영업 허가증을 반납한 뒤 추후 세금 신고 등 법적 절차를 거쳐 폐업 신고를 할 것”이라며 “경기 침체가 계속되고, 경쟁 업체가 늘어 적자 경영으로 폐업이 불가피하다”고 밝혔다.\n",
      "   6  워런은 지난해 하반기 최고 관심 후보였다. 지난해 10월 아이오와주와 뉴햄프셔주에서 여론조사 선두를 달렸다. 몇몇 전국 단위 여론조사에서는 바이든을 누르고 1위에 오르기도 했다. 같은 진보 노선을 걷는 샌더스가 심근경색으로 쓰러져 치료를 받은 영향이 컸다. 지난해 돌풍의 요인은 트럼프의 대척점에 선 정치적 입장과 정치인으로서 신선한 이미지였다.\n",
      "   7  1월 1일 새해 김정은 북한 국무위원장은 모습을 드러내지 않았다. 2012년 집권 이후 처음이다. 신년사 육성 연설은 대외 메시지인 동시에 북한 주민을 향한 연설이다. 대신 김 위원장은 나흘 짜리 전원회의를 개최해 엘리트를 단속했다. 북한이 내보냈던 방송 화면 등을 보면 노동당 청사 별관을 가득 메운 최대 1000여명으로 추정되는 북한 파워 엘리트들은 나흘 동안 '받아쓰기'에 열을 올려야 했다. 북한 매체들은 이날 오전 9시부터 신년사를 대신해 4일간의 전원회의를 총정리한 55분짜리 결과(보고)를 내보냈다.\n",
      "   8  국내 이동통신 3사(KT·SK텔레콤·LG유플러스)는 산토끼(신규 고객)를 잡느라 집토끼(장기 가입자)를 위한 서비스에 인색하다는 지적을 받아왔다. 장기 가입자는 호갱(호구+고객의 줄임말)’이라는 말이 나돌 정도다. 하지만 이동통신사들은 “회사 차원에서 장기 가입자는 VIP이며, 이들을 위한 혜택을 점차 늘리고 있다”고 반박한다.\n",
      "   9  카를로스 곤(66)닛산자동차 전 회장의 ‘보석기간 중 탈출극’으로 인한 일본내 충격이 확산되고 있다. 지난해 4월 해외 출국 금지 등을 조건으로 보석을 허가 받은 곤 전 회장은 지난달 31일 “난 레바논에 있다”는 성명을 발표했다. 레바논은 곤 전 회장 가족들의 출신지로, 곤 전 회장 역시 유년기를 그 곳에서 보냈다.\n",
      "  10  방탄소년단(BTS)이 미국 뉴욕에서 2020년 새해를 열었다. 31일(현지시간) 미국 ABC의 신년 전야 특집 프로그램인 ‘딕클락스 뉴 이어스 로킹 이브(Dick Clark’s New Year’s Rockin’ Eve)’에 출연한 방탄소년단은 뉴욕 타임스 스퀘어 무대에 올랐다. 광장 한가운데 마련된 계단에서 ‘메이크 잇 라이트(Make It Right)’ 공연을 시작한 이들은 인파 사이로 이동하면서도 흔들림 없는 노래를 선보이며 팬들의 환호성을 이끌어냈다.\n",
      "  11  카드 같은 현금 이외의 수단으로 결제한 돈이 지난해 처음으로 하루 80조원을 넘어섰다. 쓰는 돈은 많아졌는데 혜택이나 부가서비스에 만족하는 사람은 많지 않다. 특히 쓰지도 않으면서 연회비만 내는 상황에 이르렀다면 새해맞이 카드 다이어트가 필요하다. 다만 지출이 많은 직장인은 체크카드 1개, 신용카드 1개로 나눠 쓰는 게 효율적이다.\n",
      "  12  미국이 일본에 ‘유인 달 탐사 프로젝트를 함께 하자’고 정식 제안했다. '우주 굴기(崛起)'를 내세우며 세계 최초로 달의 뒷면에 탐사선을 보내는 등 빠른 속도로 우주개발을 확대하고 있는 중국을 견제하기 위해서다. 이른바 '아르테미스 계획'이다. 이후 9월 24일 방일한 브라이든스틴 NASA 국장이 일본 정부의 우주정책위원장인 가사이 요시유키(葛西敬之) JR도카이 명예회장 등을 비공식 면담하는 자리에서 “미·일 양국의 우주비행사가 월면(月面)에 함께 서는 것을 염두에 두고 (일측이) 전향적인 검토를 했으면 좋겠다”고 말한 것으로 전해진다.\n",
      "  13  올해 4월 치러지는 총선 예비후보로 지난 12월 말 기준 여야 현역 의원 8명이 등록을 마친 것으로 나타났다. 통상 정치 신인이 지역구에 얼굴을 일찍 알리는 이점을 활용하기 위해 조기에 예비후보로 등록하지만 굳이 서두를 필요가 없는 현역 의원은 늦추는 게 관례다. 하지만 이들 현역 8명은 일찌감치 예비후보 명단에 이름을 올려놓고 본격적인 선거운동에 돌입했다.\n",
      "  14  올해 수출을 비롯한 경기가 지난해보다 나아질 거라는 정부의 전망은 반도체 업황 회복 가능성에 크게 기대고 있다. 여러 전문가와 기관도 반도체 업황 회복에 따라 올해 수출이 증가세로 돌아설 거라고 내다본다. 그런 만큼 한국 경제의 ‘반도체 쏠림’현상은 심화할 것으로 보인다. 1일 산업통상자원부에 따르면 지난해 반도체 수출은 전년 대비 25. 9% 줄었다. 20대 주요 수풀 품목 중 가장 높은 감소율이다.\n",
      "  15  황 대표가 통합을 언급한 건 이번이 처음은 아니다. 그는 지난 11월 6일 “자유민주주의와 시장경제의 헌법 가치를 받드는 모든 분과의 정치적 통합을 추진하겠다”고 했고, 지난달 26일에도 병상 메시지를 통해 “흩어지고 분열해서는 저들을 막을 수 없다”고 했다. 하지만 적극적인 제스쳐라기보다 원론적인 수준에서만 통합을 거론한다는 비판이 당 안팎에서 나왔다.\n",
      "\n",
      "Source documents:\n",
      "\n",
      "  Id  sentence_per_20\n",
      "----  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "   0  40억 달러 ‘딜’ 주인공 김봉진 우아한형제들 대표태풍 뒤의 고요함이랄까.\n",
      "        40억 달러짜리 ‘딜’(거래)을 마친 뒤의 사무실은 조용했다.\n",
      "        음식 배달 앱 ‘배달의민족’ 운영사 ㈜우아한형제들의 서울 송파구 방이동 사옥은 비어 있었다.\n",
      "        전 직원들에게 연말 특별 휴가를 선물한 김봉진(44) 대표는 혼자 출근해 남은 일을 처리하고 있었다.\n",
      "        축하한다는 말을 건네자 “이제 시작인 걸요”라는 답이 돌아왔다.\n",
      "        그에게 기업 매각은 단순한 ‘엑시트’(창업 후 지분 매각으로 이익을 실현하는 일)가 아니었다.\n",
      "        성공 스토리 뒤에는 환희의 무게만큼 고민이 자리 잡고 있었다.\n",
      "        독일계 음식 배달서비스업체 DH(딜리버리 히어로)가 평가한 우아한형제들의 기업가치는 약 4조8000억원.\n",
      "        국내 스타트업 M&A 사상 최대 규모다.\n",
      "        앱 하나로 평가받은 기업가치가 GS나 현대건설의 시가총액과 맞먹는다.\n",
      "        ‘매각’이라는 표현을 썼지만, 창업자인 김 대표가 회사를 떠나는 것은 아니다.\n",
      "        오히려 역할이 커졌다.\n",
      "        두 회사가 절반씩 출자해 싱가포르에 세우는 합작법인 ‘우아DH아시아’의 책임자로서 아시아 11개국 사업을 총괄하게 된다.\n",
      "        DH는 우아한형제들의 투자자 지분 87%를 인수하고, 김 대표 등 경영진이 가진 지분 13%는 DH 본사 지분으로 전환하기로 했다.\n",
      "        그렇게 되면 김 대표는 DH 경영진 가운데 개인 최대 주주가 된다.\n",
      "         “더 큰 꿈 위해 글로벌 자본 선택” 김 대표의 고민은 ‘민족’이라는 단어에 닿아 있었다.\n",
      "        회사가 외국 자본에 넘어가면서 민족 브랜드가 어울리지 않게 됐다는 시선 때문이다.\n",
      "        소비자 반응이 긍정적이지만은 않다.\n",
      "       “겸허하게 받아들인다.\n",
      "        DH와는 경쟁 관계이지만 창업 초기부터 지속해서 교류해왔다.\n",
      "        그 과정에서 그들이 지닌 ‘글로벌 DNA’에 놀랐다.\n",
      "        DH는 홈그라운드 격인 독일 사업마저 네덜란드 기업에 넘기고 글로벌 마케팅을 강화해왔다.\n",
      "        그들과 계속 싸울지, 합쳐서 글로벌 무대로 나갈지 마지막까지 고민했다.\n",
      "        더 큰 도전을 위한 선택이라고 이해해줬으면 한다.\n",
      "       ” 국내 상장도 생각해볼 수 있었을 텐데.\n",
      "       “국내 상장이 이뤄지지 않아 아쉽긴 하다.\n",
      "        난들 여의도 거래소에서 멋있게 상장 축하 종을 쳐보고 싶지 않았겠나.\n",
      "        그러나 국내 상장이나 매각을 통해 조달할 수 있는 자본은 한계가 있었다.\n",
      "        글로벌 무대에서 경쟁하기엔 턱없이 부족한 규모라고 판단했다.\n",
      "        향후 3~4년 사업 시뮬레이션을 해 본 결과, 국내 상장으로는 ‘폼’ 한번 잡은 뒤 서서히 죽어갈 수밖에 없다는 결론을 얻었다.\n",
      "   1  시진핑 방한과 새해 한·중관계 전망지난달 23일 베이징에서 열린 한·중 정상회담을 계기로 내년 봄 시진핑(習近平) 중국 국가주석의 방한이 가시화되고 있다.\n",
      "        “시진핑 주석의 내년 상반기 방한은 확정적”이라고 청와대 관계자는 전했다.\n",
      "        6년 만에 이뤄지게 될 시 주석의 방한은 고고도미사일방어체제(THAAD·사드) 배치로 촉발된 한한령(限韓令)에 공식적으로 마침표를 찍고 한·중관계를 복원하는 계기가 될 수 있을까.\n",
      "        1992년 수교 이후 지난 27년간의 한·중관계를 되돌아보고 2020년 새해를 전망해 본다.\n",
      "             한·중 수교 이후 지금까지의 한·중관계는 경제적 협력 관계를 중심으로 발전해 왔다.\n",
      "        사드로 인해 한·중관계에 금이 가기 전까지 경제영역에서의 양자 협력은 순조로웠다.\n",
      "        이로 인해 양자 간 충돌적 요소에 대한 적극적 관리가 충분하지 않았던 것도 사실이다.\n",
      "        경험적으로 볼 때 한·중관계의 4대 충돌 요소는 ▶북핵 문제 ▶한미동맹 ▶한·중 비대칭관계 ▶반중·혐한 감정이다.\n",
      "        북핵 문제를 어떻게 접근하느냐를 두고서는 한·중간 조율이 잘된 적도 있었지만 이견을 보인 적도 많았다.\n",
      "        중국은 한미동맹에 대해 끊임없이 문제를 제기했다.\n",
      "        특히 한미동맹이 북한만을 상대하는 것이 아니라  미국이 ‘중국의 부상’을 억제하는 전략의 일부분인지가 논쟁의 대상이었다.\n",
      "        중국은 한국 정부가 바뀔 때마다 끊임없이 ‘확인’ 작업을 했다.\n",
      "         비대칭: 누가 누구를 더 필요로 하나 비대칭 관계와 반중·혐한 정서는 가끔씩 언론의 주목을 받기는 했지만 속으로 곪은 상처처럼 그 심각성에 대한 인식은 충분하지 못했다.\n",
      "        한·중관계가 평등하기보다는 중국 쪽으로 ‘운동장이 기울어진’ 비대칭관계로 서서히 바뀐 것은 격세지감을 느낄 정도로 중국의 위상이 높아짐에 따른 양자관계 변화가 저변에 깔려있다.\n",
      "        1992년 수교 당시 비슷했던 양국 경제규모는 2018년 중국이 한국의 8배가 될 정도로 격차가 벌어졌다.\n",
      "        한국이 중국을 필요로 하는 정도와 중국이 한국을 필요로 하는 정도 사이에 차이가 더 커진 원인이다.\n",
      "        한국 대통령 특사에 대한 중국 지도자의 하대 논란이 반복되는 것도 이와 무관치 않다.\n",
      "        바로잡지 않으면 ‘뉴노멀(New Normal)’로 굳어진다.\n",
      "        더불어 한미동맹 문제 등 근래 한국과 주요 우방과의 관계 악화, 그리고 세계무대에서 한국의 경제 지위 하강은 중국이 생각하는 한국의 몸값을 더욱 낮게 보이게 할 수 있다.\n",
      "   2  한·중이 사드에 관한 ‘3불(不)’이 ‘약속’인지 ‘입장표명’인지 표현을 놓고 갈등하는 모양새도 해석의 차이가 있지만, 결국은 중국이 한국을 압박할 수 있는 카드다.\n",
      "        예를 들어 미국이 중거리 미사일을 한국에 배치하면 중국은 ‘3불 위반’으로 몰아갈 수 있다.\n",
      "        한·중관계는 다시 악화될 것이고, 그 파동은 분명 사드 때보다 더 큰 양자 관계 파열이 될 것이다.\n",
      "        1962년 쿠바 미사일 위기가 회자되는 이유다.\n",
      "         착시: 시 주석 다녀가면 사드 갈등 끝난다? 2020년은 한·중관계 ‘회복’과 ‘비대칭화’가 동시에 진행되는 한 해가 될 수 있다.\n",
      "        한국이 기존 동맹인 미국 등 주요국들과 관계가 악화되면서 중국 쪽으로 외교 방향이 기울게 되는 측면과, 미·중관계 악화 속에서 한국을 중국 쪽으로 견인하려는 중국의 계산이 맞아떨어져 가는 측면이 교집합을 형성하기 때문이다.\n",
      "        미·중관계 악화가 심화되는 배경 속에서 한국이 동맹인 미국과의 관계, 이웃인 중국과의 관계를 조율시키는 숙제는 2020년에도 지속될 것이다.\n",
      "        사드 배치를 둘러싸고 한국이 보인 우유부단한 태도는 미국 내에서 일고 있는 한국의 ‘중국 경사론’과 맞물려 한국이 신뢰할 수 있는 동맹인지에 대한 문제를 야기했다.\n",
      "        한국이 결국 미국이 원하는 대로 사드를 배치했음에도 불구하고 미국의 갈채를 받지 못한 이유다.\n",
      "        미국은 한국이 사드 배치로 인해 중국의 보복을 받고 있는 와중에서도 ‘전략적 인내’를 했다.\n",
      "        이는 아무것도 안한 것을 고상하게 일컫는 말이다.\n",
      "        친구가 나 때문에 맞고 있는데 뒷짐 지고 구경한 격이다.\n",
      "        이는 미·중 사이에서 한국의 포지셔닝 전략에 대해 한국이 좀 더 깊은 고민을 해야한다는 것을 시사한다.\n",
      "        단순히 미·중 사이의 기계적인 중립이나, ‘미·중 둘 다 중요하니 어느 쪽도 선택하면 안된다’는 ‘영리한 변명’(clever excuse) 뒤에 숨어서는 안된다.\n",
      "        주변국 줄세우기는 강대국들의 오랜 역사적 패권 행동 양식이다.\n",
      "        가장 우려되는 것은 이러한 환경에서 한국이 선택할 수 있는 전략적 옵션이 마땅치 않다는 것이다.\n",
      "        이는 한국이 미·중 모두로부터 구애를 받기보다는, 양쪽 모두로부터 전략적 의구심을 받고 있는 상황과 맞물려 한국 외교의 어려움을 가중시킬 수 있다.\n",
      "        종합적으로 볼 때 한·중관계가 회복되고 한·중 간 최대 장애물이었던 사드 갈등이 마무리될 것처럼 보이는 것은 ‘착시’현상이다.\n",
      "        미·중 갈등 심화과정에서 중국이 한국에 대한 전략적 조정에 나선 측면이 더 크다.\n",
      "   3  배달의민족이 독일 자본에 매각된 것을 놓고 말들이 많다.\n",
      "        민족 정서를 배반했다며 ‘배다른 민족’ ‘게르만 민족’이라 부르자는 분노가 터져 나온다.\n",
      "        하지만 창업 9년 만에 매출액 3193억원, 영업이익 586억원의 벤처가 무려 40억 달러(약 4조7500억원)에 매각된 것은 디지털 혁명 시대를 알리는 가장 분명한 신호다.\n",
      "        디지털 전도사인 성균관대 최재붕 교수는 요즘 보람 튜브를 자주 입에 올린다.\n",
      "        6살짜리 보람이는 1인 유튜브로 한 해 수십억 원을 벌고, 서울 청담동에 95억원 짜리 빌딩도 샀다.\n",
      "        그러자 SBS의 ‘그것이 알고 싶다’는 아동학대라며 공격했다.\n",
      "        기존 잣대로라면 방송통신위와 여성가족부의 칼질로 보람 튜브는 벌써 망했어야 했다.\n",
      "        하지만 전 세계 3300만 아이들을 열광시키며 세계 1위 키즈 방송국으로 발돋움했다.\n",
      "        오히려 어른들이 만드는 KBS ‘TV 유치원’ 시청률은 고작 0.\n",
      "       2%다.\n",
      "        최 교수는 “어른들의 낡은 잣대로 아이들의 미래를 짓밟고 있다”고 비판한다.\n",
      "        혁신에는 예상외로 학벌이 별 필요가 없다.\n",
      "        기존의 비지니스 문법과 전혀 다르다.\n",
      "        기업가치 1조원 이상인 유니콘 창업자만 살펴 보자.\n",
      "        배달의민족 김봉진 대표는 공업고교를 나와 2년제 전문대를 다녔다.\n",
      "        숙박 앱인 ‘야놀자’의 이수진 대표도 공고와 2년제 전문대를 나와 모텔 청소부를 하며 꿈을 키웠다.\n",
      "        ‘여기어때’의 심명섭 대표 역시 공고-전문대를 나왔다.\n",
      "        대학 출신이라 해도 SKY 같은 명문대는 거의 없다.\n",
      "        온라인 패션 강자인 무신사의 조만호 대표는 단국대 패션디자인학과를 나와 ‘무진장 신발 사진이 많은 곳(무신사)’으로 기적을 일구었다.\n",
      "        BB크림인 닥터 자르트로 화장품 신화를 쓴 이진욱 대표도 지방의 원광대 건축학과를 나왔다.\n",
      "        모두 정통 엘리트 코스와 거리가 멀다.\n",
      "        이처럼 혁신 경제에는 개천에서 용이 나는 게 다반사다.\n",
      "        누가 창의적으로 소비자 마음을 사로잡느냐가 중요할 뿐이다.\n",
      "        그래서인지 문재인 대통령도 요즘 소득주도성장 대신 혁신성장을 부쩍 강조하고 있다.\n",
      "        전국을 돌며 “간섭도, 규제도 하지 않겠다”고 다짐한다.\n",
      "        문제는 정부의 말과 행동이 다르다는 점이다.\n",
      "        알맹이 빠진 혁신은 껍데기나 다름없다.\n",
      "        정부는 승차 공유의 타다에 대해 “택시업계와의 상생방안을 마련하라”고 강요했다.\n",
      "        숙박 공유에 대해선 “허용 범위와 투숙객 안전 확보를 위해 제도 정비가 우선”이라며 기존 법률을 핑계 삼아 사실상 금지시켰다.\n",
      "   4  지난 28일부터 나흘간 진행된 7기 5차 노동당 전원회의에서 북한은 핵 무력 개발의 주역인 이병철 군수공업부 제1부부장을 정치국 위원과 당 부위원장, 부장(군수공업부)에 임명했다.\n",
      "        정치국 위원은 평상시 노동당의 정책을 결정하는 정치국 회의에서 투표권을 행사하는 자리로, 노동당의 핵심 인사로 꼽힌다.\n",
      "        20명 안팎의 인원으로 구성돼 있다.\n",
      "         이병철은 북한이 핵 무력 완성을 선언한 2017년 11월 29일 화성-15형 장거리 미사일 발사를 비롯해 지난해 13차례의 단거리 발사체 실험을 챙겨온 미사일 분야 전문가로, 군수공업부 제1부부장을 맡아 왔다.\n",
      "        그는 김 위원장의 미사일 발사 현장 참관 때 동행하곤 했다.\n",
      "          김 위원장은 이번 회의에서 \"국가의 힘, 국방력 강화에서 거대한 성과들을 끊임없이 비축했다\"며 \"첨단 무기체계들을 개발하는 방대하고도 복잡한 사업이 우리의 믿음직한 과학자, 설계가, 군수노동계급에 의해 완벽히 수행됐다\"고 평가했다.\n",
      "        그런 점에서 기존 태종수의 자리에 이 분야 책임자인 이병철을 기용한 것으로 보인다.\n",
      "         이병철과 함께 이일환 당 근로단체부장(선전선동부장 이동 가능성)과 김덕훈 내각 부총리를 정치국 위원 겸 당 부위원장으로 선출했다.\n",
      "        이들은 각각 태종수, 박광호, 노두철의 자리를 물려받은 것으로 추정된다.\n",
      "         \"타성에서 못 벗어난 경제 관리\"…경제 부총리 철퇴?  김덕훈 내각 부총리의 정치국 위원 임용은 노두철 내각 부총리(국가계획위원장 겸직) 자리를 대신한 것으로 보인다.\n",
      "        기존 정치국 위원이었던 노두철은 회의 기간 내내 주석단에 모습을 보이지 않아 경질설이 제기됐다.\n",
      "        특히 김 위원장은 전원회의에서 \"국가 관리와 경제사업을 비롯한 이여의(다른) 분야에서 바로잡아야 할 문제가 적지 않다”며 “자력갱생, 자급자족하자고 계속 말하고 있지만 이를 실행하는 우리의 사업은 지난날의 타성에서 탈피하지 못하고 있다”고 질책했다.\n",
      "        북한이 이날 국가계획위원장을 김일철로 교체했다고 밝힌 점도 노두철 경질을 추정케 하는 대목이다.\n",
      "            전원회의에 나오지 않아 신변이상설이 돌았던 박봉주 당 부위원장은 김 위원장이 회의 참석자들과 사진촬영을 하는 자리에 휠체어를 타고 등장했다.\n",
      "        그는 지난달 27일(보도일) 상원시멘트연합기업소를 현장 지도했고, 이번 회의에서 서면 토론을 진행했는데 막상 회의 석상에는 나타나지 않다가 휠체어를 타고 모습을 드러낸 것이다.\n",
      "        일각에선 그가 지방에서 회의 참석차 평양으로 향하다 교통사고를 당한게 아니냐는 얘기가 돈다.\n",
      "   5  부산 해운대의 상징이었던 5성급 해운대그랜드호텔(그랜드호텔)이 지난해 12월 31일 폐업했다.\n",
      "        그랜드호텔측은 호텔고급화 경쟁에 밀려 적자 경영으로 폐업이 불가피하다고 한다.\n",
      "        호텔업계에서는 해운대 호텔 간의 생존 경쟁을 알리는 신호탄으로 보고 있다.\n",
      "        반면 그랜드호텔 노조는 고용 승계를 피하려는 위장 폐업이라고 주장한다.\n",
      "         그랜드호텔측은 1일 “영업 허가증을 반납한 뒤 추후 세금 신고 등 법적 절차를 거쳐 폐업 신고를 할 것”이라며 “경기 침체가 계속되고, 경쟁 업체가 늘어 적자 경영으로 폐업이 불가피하다”고 밝혔다.\n",
      "        그랜드호텔은 2016년 39억, 2017년 27억원의 순이익을 냈지만, 2018년에는 3억9000만원의 적자를 기록했다.\n",
      "        호텔 폐업을 위해서는 구청에 폐업 신고를 해야 한다.\n",
      "         반면 이 호텔 노조는 2일부터 호텔 내 노조 사무실을 점거, 출근 투쟁을 이어갈 방침이다.\n",
      "        황순원 민주노총 서비스연맹 사무국장은 중앙일보와 통화에서 “사측이 폐업신고를 하지 않고 영업만 종료한 상태”라며 “정관에 노조 합의 없이 폐업할 수 없다고 명시돼 있고, 고용 관계가 종료되지 않았다.\n",
      "        계속 출근하면서 사측과 상생 방안을 논의할 것”이라고 말했다.\n",
      "           그랜드 호텔이 적자를 낸 원인은 호텔 간의 고급화 경쟁에서 밀렸기 때문이라는 관측이 나온다.\n",
      "        고급화 경쟁에 불을 지핀 것은 2017년 7월 부산 기장군에 들어선 아난티 코브다.\n",
      "        1㎞가 넘는 해안가를 따라 약 7만6033㎡(2만3000평) 규모의 대지 위에 들어선 아난티 코브는 성수기 때 해운대 기존 호텔보다 비싼 가격대임에도 만실 행진을 이어갔다.\n",
      "        호텔업계에서 아난티코브가 이례적으로 오픈 첫해 흑자를 기록하자 대책 마련에 부심했다.\n",
      "          오래된 호텔은 리모델링에 나섰다.\n",
      "        5성급 파라다이스호텔은 2017년 상반기 700억원을 투자해 리모델링을 마쳤다.\n",
      "        시설 노후로 특급호텔 위상을 내려놓을 위기에 처했던 노보텔부산은 지난해 초 리모델링 공사에 들어갔다.\n",
      "        노보텔부산은 오는 7월 신세계조선호텔로 다시 문을 열 계획이다.\n",
      "          동백섬에 있는 부산웨스틴조선호텔도 올해 전체 리모델링을 계획하고 있다.\n",
      "        1978년 개관한 이 호텔은 기존 290실인 객실 규모를 반으로 줄이는 대신 6성급 호텔로 고급화할 계획인 것으로 전해졌다.\n",
      "          해운대해수욕장 끝자락에 짓고 있는 엘시티에 들어설 롯데 시그니엘 호텔은 해운대 호텔업계에 본격적인 고급화 경쟁을 몰고 올 전망이다.\n",
      "   6  본선에서 트럼프에 패할 수 있다는 우려 때문이다.\n",
      "        지지율 3위: 엘리자베스 워런 ◈강점◈ 워런은 지난해 하반기 최고 관심 후보였다.\n",
      "        지난해 10월 아이오와주와 뉴햄프셔주에서 여론조사 선두를 달렸다.\n",
      "        몇몇 전국 단위 여론조사에서는 바이든을 누르고 1위에 오르기도 했다.\n",
      "        같은 진보 노선을 걷는 샌더스가 심근경색으로 쓰러져 치료를 받은 영향이 컸다.\n",
      "         지난해 돌풍의 요인은 트럼프의 대척점에 선 정치적 입장과 정치인으로서 신선한 이미지였다.\n",
      "        트럼프식 정치에 지친 유권자들은 워런이 새로운 대안이 될 수 있을지 관심을 갖기 시작했다.\n",
      "        특히 고학력 백인의 지지가 컸다.\n",
      "         아이오와주에서 지지층이 두껍고 지지율도 높다.\n",
      "        여세를 몰아 뉴햄프셔주에서 좋은 성적을 거둔 뒤 '슈퍼 화요일' 경선 주이자 고향인 매사추세츠주에도 깃발을 꽂으면 대선을 향한 길이 활짝 열릴 수 있다.\n",
      "        화제성에서 2위를 달리고 있다.\n",
      "        ◈약점◈ 일련의 정책 공약을 발표하면서 지나치게 왼쪽으로 치우쳤다는 평가를 받는다.\n",
      "        전 국민 의료보험 공약은 재원 마련 방법에 대한 비판이 쏟아지자 대통령 임기 3년 차 이전에는 추진하지 않겠다고 한발 물러서야 했다.\n",
      "         이 과정에서 일부 지지층이 부티지지 쪽으로 옮겨갔다는 분석이 있다.\n",
      "        부티지지와 고학력 백인 유권자를 놓고 경쟁하는 구도다.\n",
      "        샌더스와는 급진 좌파 성향 유권자를 놓고 다투고 있다.\n",
      "        지난해 7월 RCP 조사 1위에서 올해 3위로 내려앉았다.\n",
      "        두각을 나타내자 그에 대한 검증이 본격화했기 때문이다.\n",
      "        경쟁자의 공격도 거세졌다.\n",
      "        지지율 4위: 피트 부티지지  ◈강점◈좀처럼 흥행이 안 되는 이번 대선에서 '신선함'을 담당한 후보다.\n",
      "        올해 38세로 혁신적 이미지 구축에 성공했다.\n",
      "        미 중서부 인디애나주의 작은 도시인 사우스밴드 시장에서 일약 민주당 후보 빅4로 발돋움했다.\n",
      "        떠오르는 혜성이다.\n",
      "        언론을 통해 다방면의 깊은 조예와 풍부한 학식을 보여주며 고학력 백인 유권자의 지지를 확보했다.\n",
      "        하버드대와 옥스퍼드대를 졸업한 뒤 맥킨지에서 경영 컨설턴트로 일했다.\n",
      "         정치적 입장과 성향은 중도를 지향한다.\n",
      "        한마디로 '젊고 에너지 넘치는 바이든'이다.\n",
      "        29세에 사우스밴드 시장에 당선돼 33세에 재선됐다.\n",
      "        바이든의 약점인 '고령'은 극복했지만, 강점인 '경험'은 부족한 것으로 평가받는다.\n",
      "         미국 주요 정당 대선 후보 가운데 처음으로 커밍아웃한 후보다.\n",
      "   7  1월 1일 새해 김정은 북한 국무위원장은 모습을 드러내지 않았다.\n",
      "        2012년 집권 이후 처음이다.\n",
      "        신년사 육성 연설은 대외 메시지인 동시에 북한 주민을 향한 연설이다.\n",
      "        대북 제재 하에서 그동안 자신이 약속했던 경제 개발과 대미 협상의 성과가 부진했던 탓이라는 분석이 나오는 이유다.\n",
      "         대신 김 위원장은 나흘 짜리 전원회의를 개최해 엘리트를 단속했다.\n",
      "        북한이 내보냈던 방송 화면 등을 보면 노동당 청사 별관을 가득 메운 최대 1000여명으로 추정되는 북한 파워 엘리트들은 나흘 동안 '받아쓰기'에 열을 올려야 했다.\n",
      "        북한 매체들은 이날 오전 9시부터 신년사를 대신해 4일간의 전원회의를 총정리한 55분짜리 결과(보고)를 내보냈다.\n",
      "        핵심 구호는 김 위원장 스스로 밝힌 대로 \"우리의 전진을 저애(저해)하는 모든 난관을 정면돌파전으로 뚫고 나가자\"였다.\n",
      "         김 위원장은 지난해 신년사에서 '새로운 길'을 꺼낼 수 있다고 언급한 데 이어, 하노이 북·미 2차 정상회담 결렬 후 열린 지난해 4월 최고인민회의에서 '연말시한'을 정하기도 했다.\n",
      "        하지만 이날 전원회의 결과는 '과거의 길'이었다.\n",
      "        미국의 대북 적대시 정책에 굴복하지 않겠다는 것과 대북 제재로 어려워진 경제 여건을 자력갱생을 통해 정면 돌파하겠다는 것이다.\n",
      "        '정면 돌파'라는 표현은 모두 22차례나 사용했다.\n",
      "        경제 개발 부진의 책임을 물은 듯 당 부장의 절반 이상을 교체하는 등 대규모 인사도 단행했다.\n",
      "        전문가들 사이에선 김 위원장이 '핵·경제 병진 노선'으로 회귀했다거나, 김정일 시대로 돌아간 것 아니냐는 평가가 나오고 있다.\n",
      "        결과 보고에서 김 위원장은 \"세상은 머지않아 새로운 전략무기를 목격하게 될 것\"이라고 주장했다.\n",
      "        \"파렴치한 미국이 조·미(북·미) 대화를 불순한 목적 실현에 악용하는 것을 절대로 허용하지 않을 것\"이라며 \"인민이 당한 고통과 억제된 발전의 대가를 깨끗이 다 받아내기 위한 충격적인 실제 행동에 나설 것\"이라고 강조하면서다.\n",
      "        김 위원장의 이날 발언 중 가장 강한 표현이었다.\n",
      "            하지만 거기까지였다.\n",
      "        \"핵 군축과 전파 방지를 위한 노력에 찬물을 끼얹었다\"고 말했지만, 명시적으로 핵실험과 미사일 시험발사 중단(모라토리엄)을 선언하진 않았다.\n",
      "        나아가 \"우리의 억제력 강화의 폭과 심도는 미국의 금후 대조선 입장에 따라 상향 조정될 것\"이라고 언급, 향후 미국과의 협상 여지를 열어놓았다.\n",
      "         기다렸다는 듯이 도널드 트럼프 미국 대통령은 지난달 31일 \"김 위원장은 약속을 지킬 것\"이라며 부드러운 반응을 내놓았다.\n",
      "   8  국내 이동통신 3사(KT·SK텔레콤·LG유플러스)는 산토끼(신규 고객)를 잡느라 집토끼(장기 가입자)를 위한 서비스에 인색하다는 지적을 받아왔다.\n",
      "        장기 가입자는 호갱(호구+고객의 줄임말)’이라는 말이 나돌 정도다.\n",
      "        하지만 이동통신사들은 “회사 차원에서 장기 가입자는 VIP이며, 이들을 위한 혜택을 점차 늘리고 있다”고 반박한다.\n",
      "        장기 가입자를 위한 이통 3사의 서비스를 비교해봤다.\n",
      "        무료 데이터 쿠폰, LG유플러스〉SK텔레콤〉KT통신업계에 따르면 이통사의 장기가입자로 분류되는 기준은 '가입 기간 2년 이상'이다.\n",
      "        이통사는 이때부터 장기 가입자용 서비스를 제공한다.\n",
      "        가장 일반적인 서비스는 무료 데이터 쿠폰’이다.\n",
      "        소비자가 가입한 요금제에서 기본으로 받는 데이터 용량만큼 더 쓸 수 있도록 쿠폰을 추가로 발급해준다.\n",
      "        이통 3사 가운데 무료 데이터 쿠폰을 가장 많이 주는 곳은 LG유플러스다.\n",
      "        2년 이상 가입자에게 데이터 쿠폰 5장을 준다.\n",
      "        이 경우 소비자는 자신의 요금제에 포함된 데이터를 다 쓴 뒤, 쿠폰으로 5배까지 더 쓸 수 있다.\n",
      "        3년 가입자 6장, 4년 이상일 때는 7장을 지급한다.\n",
      "         SK텔레콤은 2년 이상 가입자에게 데이터 쿠폰 4장, 3년 이상이면 5장, 4년이 넘으면 6장을 준다.\n",
      "        KT는 2년 이상 가입자에게 데이터 쿠폰 4장, 4년 이상이면 6장 제공한다.\n",
      "        KT 관계자는 \"타사는 소비자가 가입한 요금제에 맞춰 데이터를 제공하는 반면, KT는 요금제 관계없이 쿠폰 한 장당 2GB씩 제공한다\"면서 \"쿠폰 수는 적지만 저가 요금제 쓰는 소비자에게는 오히려 더 많은 데이터를 제공한 것\"이라고 설명했다.\n",
      "        가족결합할인, LG유플러스의 할인 폭 가장 커 ‘가족결합 할인 제도’도 장기 가입자를 위한 대표적 서비스다.\n",
      "        가족 간 이동전화 가입 기간을 합산해 요금을 할인해주는 제도다.\n",
      "        LG유플러스의 'U+가족무한사랑' 요금제를 사용하면 가족 구성원 2~4명이 한꺼번에 요금 할인을 받을 수 있다.\n",
      "        같은 명의로 휴대전화 번호가 2개 개설돼 있어도 가족결합으로 할인받을 수 있고, 일시 정지 중인 번호도 결합할 수 있다.\n",
      "        번호를 결합한 가족 구성원끼리 LG유플러스를 사용한 기간을 합산해 15년이 넘으면 가구당 1만1000원, 30년 이상이면 2만2000원을 할인해준다.\n",
      "        이에 더해 구성원별로 최대 5500원씩 요금할인을 추가로 받을 수 있다.\n",
      "        가족 4명이 합산한 가입 기간이 30년이 넘는 경우, 매월 4만4000원의 통신비를 줄일 수 있는 셈이다.\n",
      "   9  카를로스 곤(66)닛산자동차 전 회장의 ‘보석기간 중 탈출극’으로 인한 일본내 충격이 확산되고 있다.\n",
      "        지난해 4월 해외 출국 금지 등을 조건으로 보석을 허가 받은 곤 전 회장은 지난달 31일 “난 레바논에 있다”는 성명을 발표했다.\n",
      "         레바논은 곤 전 회장 가족들의 출신지로, 곤 전 회장 역시 유년기를 그 곳에서 보냈다.\n",
      "            일본 언론들에 따르면 보석을 신청했던 변호사측은 “아닌 밤중에 홍두깨다.\n",
      "        전혀 몰랐다”고 황당해했다.\n",
      "        보석을 허가한 법원 역시  “쇼크”라며 망연자실한 분위기다.\n",
      "         산케이 신문은 “증거인멸과 도주 가능성을 이유로 보석에 강하게 반대했던 도쿄지검 특수부에선 ‘언젠가는 도망갈 줄 알았다’는 얘기가 나온다”고 전했다.\n",
      "        한 검찰 간부는 이 신문에 “일본의 형사사법 제도의 부끄러움을 전세계에 알린 법원과 변호인단은 책임이 무겁다”고 양측을 강하게 비판했다.\n",
      "        이런 가운데 곤 전 회장의 일본 탈출의 구체적인 경위와 관련된 보도가 잇따르고 있다.\n",
      "          일본 언론들에 따르면 레바논의 주요 미디어인 MTV는 “곤 전 회장이 악기 상자에 숨어 출국했다”고 보도했다.\n",
      "         민간경비회사 또는 ‘군대에 준하는 민병조직’ 관계자들이 크리스마스 만찬음악회의 밴드로 가장해 곤 전 회장의 도쿄 거처에 들어갔고,연주를 끝낸 뒤 곤 전 회장을 악기 상자에 숨겨 나왔다는 것이다.\n",
      "         산케이 신문은 \"취재원을 밝히지 않았기 때문에 보도의 신뢰성은 명확하지 않다\"고 했다.\n",
      "        곤 전 회장은 자가용 비행기편으로 지난달 29일 오사카 간사이 국제공항을 출발해 터키를 거쳐 지난달 30일(현지시간)레바논에 도착했을 가능성이 크다고 한다.\n",
      "           이와관련, 아사히 신문은 일본 국토교통성을 인용해 “자가용 비행기를 사용할 경우에도 일반 여객과 마찬가지로 CIQ(세관·출입국관리·검역)를 받을 필요가 있어 CIQ 검사를 받지 않고 출국했을 가능성은 100% 불가능하다”고 했다.\n",
      "        하지만 X선 화물 검사에 대해선 “자가용 비행기의 경우 의무화돼 있지 않고, 기장 등의 판단으로 생략되는 경우도 있다”고 전했다.\n",
      "        레바논 TV와 아사히 신문 보도를 종합하면 곤 전 회장이 숨어있었던 악기 박스가 오사카 공항에서 수화물 검사를 받지 않고 자가용 비행기에 실렸을 가능성이 있다.\n",
      "        유럽 언론들은 일련의 탈출 계획을 주도한 인물로 곤 전 회장의 부인 캐럴을 지목하고 있다.\n",
      "         프랑스의 르몽드는 “터키와 양호한 관계를 가진 형제들과 함께 캐럴이 탈출 준비를 진행했을 가능성이 크다”고 전했다.\n",
      "  10  방탄소년단(BTS)이 미국 뉴욕에서 2020년 새해를 열었다.\n",
      "        31일(현지시간) 미국 ABC의 신년 전야 특집 프로그램인 ‘딕클락스 뉴 이어스 로킹 이브(Dick Clark’s New Year’s Rockin’ Eve)’에 출연한 방탄소년단은 뉴욕 타임스 스퀘어 무대에 올랐다.\n",
      "        광장 한가운데 마련된 계단에서 ‘메이크 잇 라이트(Make It Right)’ 공연을 시작한 이들은 인파 사이로 이동하면서도 흔들림 없는 노래를 선보이며 팬들의 환호성을 이끌어냈다.\n",
      "        본무대에 올라 ‘작은 것들을 위한 시’를 부른 이들은 “해피 뉴 이어”라고 새해 인사를 건넸다.\n",
      "        이날 진행을 맡은 방송인 라이언 시크레스트는 방탄소년단을 “세계적인 현상의 주인공”이자 “전 지구를 홀린 그룹”이라고 소개했다.\n",
      "        지난해 4월 발매한 미니앨범 ‘맵 오브 더 솔: 페르소나’로 미국ㆍ영국ㆍ일본 등 전 세계 음악 차트를 석권한 데 이어 미국 3대 음악상인 아메리칸 뮤직 어워드 3관왕, 빌보드 뮤직 어워드 2관왕에 오르는 등 새로운 아이콘으로서 이뤄낸 성취를 높이 평가한 것이다.\n",
      "        특히 올해는 2019년 한 해뿐만 아니라 2010년대 10년을 마무리하고 2020년을 맞이하는 행사라 더욱 뜻깊었다.\n",
      "        방탄소년단은 이날 포스트 말론 등과 함께 무대에 올라 볼 드롭을 지켜보며 새해맞이 카운트다운을 외쳤다.\n",
      "        RM은 “여섯 살 때부터 영화 ‘나 홀로 집에’를 보며 지켜보던 광경이 눈앞에 펼쳐지고 있다”고 소감을 밝혔다.\n",
      "        라이언 시크레스트가 “보통 어떻게 새해를 맞이하냐”고 묻자 RM은 “가족이나 친구들과 함께 크리스마스 트리 앞에 모여서 맛있는 음식을 먹으며 새해를 위한 각오를 다진다”고 답했다.\n",
      "        방탄소년단의 미국 새해맞이 행사 나들이는 이번이 두 번째다.\n",
      "        2017년에는 사전녹화를 통해 로스앤젤레스에서 ‘DNA’와 ‘마이크 드롭(MIC DROP)’ 무대를 선보였다.\n",
      "        2012년 싸이가 한국 가수 최초로 ‘강남스타일’을 선보인 이후 방탄소년단이 두 번째 무대를 장식하면서 K팝에 대한 관심을 높였다.\n",
      "        12월 31일 밤부터 1월 1일 새벽까지 생중계되는 이 행사는 매년 100만명 이상의 인파가 몰리고, 미국 전역에서 2500만여명이 시청한다.\n",
      "        올해 행사는 뉴욕ㆍ뉴올리언스ㆍ로스앤젤레스ㆍ마이애미 등 4곳에서 진행됐다.\n",
      "        가수 시애라가 진행을 맡은 로스앤젤레스에서는 첫 무대를 연 두아리파에 이어 파울라압둘 등의 공연이 이어졌다.\n",
      "  11  매일 커피전문점을 찾는 이지은(32) 씨는 결제할 때마다 5초쯤 멈춘다.\n",
      "        카드를 고르기 위해서다.\n",
      "        스마트폰 간편결제를 할 때도 화면을 넘기며 오늘의 카드를 고르는 건 마찬가지다.\n",
      "        직장 생활을 시작하면서 하나둘씩 만들었는데 지갑 속에 꽂힌 것만 9장.\n",
      "        이 씨는 “발급할 땐 분명 이유가 있었을 텐데 이젠 기억이 안 난다”며 “쓰지도 못할 카드를 뭐하러 이렇게 만들었는지 나조차 의아하다”고 말했다.\n",
      "        한국은행에 따르면 국내 신용카드 발급량은 2008년 금융위기 이후 2014년까지 꾸준히 줄었다.\n",
      "        하지만 2015년부터 늘어 지난해 다시 1억장을 돌파했다.\n",
      "        체크카드(1억3000만장)까지 합하면 대략 2억4000만장.\n",
      "        성인 인구로 단순 계산해도 1인당 5~6장의 카드를 쓴다는 얘기다.\n",
      "        개수만 는 게 아니다.\n",
      "        현금을 덜 쓰고, 카드를 더 쓰는 추세는 뚜렷하다.\n",
      "        카드 같은 현금 이외의 수단으로 결제한 돈이 지난해 처음으로 하루 80조원을 넘어섰다.\n",
      "        쓰는 돈은 많아졌는데 혜택이나 부가서비스에 만족하는 사람은 많지 않다.\n",
      "        이 씨처럼 만들 땐 다 이유가 있다.\n",
      "        쇼핑몰 할인율이 높아서, 포인트를 많이 쌓아줘서, 연회비가 없어서.\n",
      "        그럼 만든 목적대로 써야 하는데 생각 같지 않다.\n",
      "        뭐든 숫자가 늘면 관리가 힘든 법이다.\n",
      "        특히 쓰지도 않으면서 연회비만 내는 상황에 이르렀다면 새해맞이 카드 다이어트가 필요하다.\n",
      "        고민할 것 없이 딱 한 장으로 줄이면 간편하다.\n",
      "        다만 지출이 많은 직장인은 체크카드 1개, 신용카드 1개로 나눠 쓰는 게 효율적이다.\n",
      "        각각의 장점이 있어서다.\n",
      "        일단 연말정산에 도움이 된다.\n",
      "        카드 사용액은 연말정산 때 소득공제 혜택이 있다.\n",
      "        공제율은 신용카드가 15%, 체크카드가 30%다.\n",
      "        공제율 자체는 체크카드가 좋지만, 지출이 총급여의 25%를 넘어야 공제를 받을 수 있다.\n",
      "        혜택이 좋은 신용카드로 일정액을 쓰고, 지출이 25%를 확실히 넘긴다고 판단될 때 체크카드 사용을 늘리는 게 합리적이다.\n",
      "        이렇게 한 장씩만 남기면 생활 패턴에 맞춰 쓰기도 좋다.\n",
      "        편의점이나 커피전문점에서는 체크카드를, 고정 지출이나 큰돈 쓸 일 생겼을 땐 신용카드를 사용하는 식이다.\n",
      "        보통 체크카드는 특정 쇼핑몰이나 커피전문점에 할인 혜택을 몰아준다.\n",
      "        발급할 때부터 본인이 자주 방문하는 곳과 주거래은행에 맞추는 게 좋다.\n",
      "        전월 사용금액 기준이 까다로운 신용카드는 매월 자동결제나 할부, 택시비 등을 활용해 별다른 계산 없이도 실적을 채울 수 있도록 준비해야 한다.\n",
      "  12  미국이 일본에 ‘유인 달 탐사 프로젝트를 함께 하자’고 정식 제안했다.\n",
      "        지난해 9월 일본을 방문한 짐 브라이든스틴 미 항공우주국(NASA) 국장이 일본 정부에 이런 제안을 했다고 마이니치신문이 복수의 관계자를 인용해 1일 보도했다.\n",
      "        '우주 굴기(崛起)'를 내세우며 세계 최초로 달의 뒷면에 탐사선을 보내는 등 빠른 속도로 우주개발을 확대하고 있는 중국을 견제하기 위해서다.\n",
      "        일본 우주비행사가 달에 발을 내딛는 것은 2025년 이후가 될 전망이다.\n",
      "        계획이 성사되면 일본은 미국에 이어 두 번째로 달에 인간을 보낸 국가가 된다.\n",
      "          미국은 ‘아폴로 계획’(1961~72년)에 따라 12명의 우주비행사를 달에 보냈다.\n",
      "        이후 어떤 국가도 유인 달 탐사에 나서지 못했다.\n",
      "        ‘우주 패권’을 주창하는 도널드 트럼프 행정부는 지난 5월 ‘다시 유인 달 탐사를 시작하겠다’고 발표했다.\n",
      "        이른바 '아르테미스 계획'이다.\n",
      "         일본도 적극적이다.\n",
      "        마이니치에 따르면 지난해 5월 아베 신조(安倍晋三) 총리가 일본을 국빈 방문한 트럼프 대통령에게 아르테미스 계획 동참을 검토하겠다고 먼저 제안했다는 것이다.\n",
      "        이후 9월 24일 방일한 브라이든스틴 NASA 국장이 일본 정부의 우주정책위원장인 가사이 요시유키(葛西敬之) JR도카이 명예회장 등을 비공식 면담하는 자리에서 “미·일 양국의 우주비행사가 월면(月面)에 함께 서는 것을 염두에 두고 (일측이) 전향적인 검토를 했으면 좋겠다”고 말한 것으로 전해진다.\n",
      "        계획에 따르면 미국은 2024년 남·녀 우주비행사 2명을 시작으로 매년 유인 탐사를 전개할 예정이다.\n",
      "        아르테미스 계획에는 유인 달착륙 외에도 달기지 건설과 '게이트웨이(Gateway)'로 불리는 달궤도 우주정거장 건설까지 포함돼 있다.\n",
      "        게이트웨이는 이름 그대로 화성 탐사 등 더 깊은 우주로 떠나는 중간기지 역할도 하게된다.\n",
      "        결국 천문학적인 비용이 들 수밖에 없다.\n",
      "        이는 과거 미국이 달 탐사와 우주비행선 프로젝트를 접은 결정적인 이유다.\n",
      "        일본에 러브콜을 보내는 중요한 배경으로도 지목된다.\n",
      "        일본 정부는 지난해 10월 결정한 게이트웨이 기술·기기 제공에 대한 예산만 2024년까지 2130억 엔(약 2조2680억원)이 필요할 것으로 추산하고 있다.\n",
      "        이는 현재 일본의 국제우주탐사 관련 예산(연간 350억~400억엔)의 5배가 넘는다.\n",
      "        우주비행사를 달에 보내는 비용을 고려하면 예산은 눈덩이처럼 불어날 수 있다.\n",
      "  13  올해 4월 치러지는 총선 예비후보로 지난 12월 말 기준 여야 현역 의원 8명이 등록을 마친 것으로 나타났다.\n",
      "        통상 정치 신인이 지역구에 얼굴을 일찍 알리는 이점을 활용하기 위해 조기에 예비후보로 등록하지만 굳이 서두를 필요가 없는 현역 의원은 늦추는 게 관례다.\n",
      "        하지만 이들 현역 8명은 일찌감치 예비후보 명단에 이름을 올려놓고 본격적인 선거운동에 돌입했다.\n",
      "        1일 중앙선거관리위원회에 따르면, 지난해 12월 31일 기준 예비후보 명부에 등록된 현역 의원은 민주당 심재권·신창현·박경미·전재수·박재호·최인호 의원, 자유한국당 윤종필 의원, 대안신당 천정배 의원 등 총 8명이다.\n",
      "        이들은 새로 연 선거사무소 앞에 '기호 ◇번 예비후보 OOO'라고 적힌 대형 현수막을 내걸기도 하고 연말연시 유동인구가 많은 곳에서 유권자들에게 명함을 돌리며 눈도장을 찍고 있다.\n",
      "        예비후보로 등록하면 ▶선거사무소 설치 ▶선거운동용 명함 배부 ▶어깨띠·표지물 착용 ▶전화 지지 호소 ▶예비후보자 홍보물 발송 등의 선거운동을 할 수 있다.\n",
      "        이때문에 유권자 접촉이 급한 정치 신인들은 예비후보 등록을 서두르는 경우가 많다.\n",
      "        하지만 현역 의원이 예비후보에 조기 등록하는 건 드문 일이다.\n",
      "        민주당 한 의원은 \"연말연시 행사에 가면 축사에서 '예산 따왔다'고 홍보할 수 있지만 예비후보 등록 후에는 행사 취지에 맞는 말만 해야 해 불리할 수 있다\"고 했다.\n",
      "        이에 따라 상당수 현역 의원들은 이달 15일까지가 기한인 의정보고를 마치면 16일부터 예비후보 등록을 한다는 계획이다.\n",
      "        8명이 '현역 프리미엄'을 내려놓고 서둘러 예비후보 등록에 나선 이유는 뭘까.\n",
      "        '경쟁자 등판에 조기 총선모드'  서울 강동을이 지역구인 심재권 민주당 의원은 예비후보 등록 개시 이틀째인 지난달 18일 예비후보에 등록했다.\n",
      "        16대·19대·20대 총선 때 줄곧 이 지역에서 당선된 심 의원은 이번 예비후보 등록을 통해 21대 총선 출마 의지를 분명히 했다.\n",
      "        이해식 민주당 대변인도 같은 날 이 지역구에 예비후보 등록을 마쳤다.\n",
      "        당내에서는 강동구청장 출신 이 대변인과 심 의원의 대결이 치열한 승부가 될 거란 전망이 많다.\n",
      "        민주당 관계자는 \"심 의원이 예비후보에 등록했다는 것은 당내 경선에서부터 지역구를 반드시 사수하겠다는 뜻 아니겠느냐\"고 풀이했다.\n",
      "        신창현 민주당 의원은 지난달 17일 예비후보 등록을 마쳤다.\n",
      "        신 의원 지역구인 경기 의왕-과천에는 신 의원을 포함해  민주당 예비후보 4명이 등록했다.\n",
      "  14  “반도체 가격이 상승 전환될 것이라고 보는 게 글로벌 전망 기관의 지배적 견해다.\n",
      "       ”(홍남기 부총리 겸 기획재정부 장관.\n",
      "        지난해 12월 30일 기자 간담회)“2020년 반도체 시황은 제한적인 공급 증가와 수요 개선에 따라 2019년보다 좋을 것”(산업통상자원부 1월 1일.\n",
      "        ‘수출입 동향’보도자료) 정부의 믿는 구석은 결국 반도체뿐이다.\n",
      "        올해 수출을 비롯한 경기가 지난해보다 나아질 거라는 정부의 전망은 반도체 업황 회복 가능성에 크게 기대고 있다.\n",
      "        여러 전문가와 기관도 반도체 업황 회복에 따라 올해 수출이 증가세로 돌아설 거라고 내다본다.\n",
      "        그런 만큼 한국 경제의 ‘반도체 쏠림’현상은 심화할 것으로 보인다.\n",
      "          1일 산업통상자원부에 따르면 지난해 반도체 수출은 전년 대비 25.\n",
      "       9% 줄었다.\n",
      "        20대 주요 수풀 품목 중 가장 높은 감소율이다.\n",
      "        한국 경제의 대표 주자인 반도체 수출이 확 꺾이자 전체 수출도 크게 줄었다.\n",
      "        지난해 수출액은 전년 대비 10.\n",
      "       3% 감소했다.\n",
      "        글로벌 금융위기 여파가 있던 2009년 이후 첫 두 자릿수(-13.\n",
      "       9%) 감소율이다.\n",
      "        지난해 수출 감소의 주범인 반도체가 올해는 다시 수출 호전의 주역이 될 거라고 정부는 기대하고 있다.\n",
      "        징후는 보인다.\n",
      "        물량 기준으로 보면 반도체 수출은 지난해 7월부터 12월까지 6개월째 늘고 있다.\n",
      "        문제는 수출 단가인데, 요즘 추세를 보면 긍정적이다.\n",
      "        시장조사기관인 D램익스체인지에 따르면 PC용 DDR4 8기가비트(Gb) D램의 지난해 12월 고정거래가격은 2.\n",
      "       81달러다.\n",
      "        전달과 같은 수준을 기록하며 하락세가 멈춰섰다.\n",
      "        낸드플래시는 이미 오름세다.\n",
      "        128Gb MLC 낸드플래시 가격은 12월 4.\n",
      "       42달러로 한 달 전보다 2.\n",
      "       55% 상승했다.\n",
      "        시장 반도체 가격은 수출 단가와 직결된다.\n",
      "         다른 경제 지표에선 이미 반도체 업황 회복에 따른 반등이 나타났다.\n",
      "        지난해 11월 전(全) 산업생산은 한 달 전보다 0.\n",
      "       4% 늘어나며 오름세로 돌아섰다.\n",
      "        반도체 생산이 전월 대비 9.\n",
      "       3% 늘어난 덕을 봤다.\n",
      "        기업 심리도 반도체 업황 개선 기대에 다소 나아졌다.\n",
      "        지난해 12월 전 산업의 업황 BSI(기업경기실사지수)는 전월보다 2포인트 상승한 76으로 집계됐다.\n",
      "         수출에도 곧 반도체 회복이 반영될 거라는 게 정부의 기대 섞인 전망이다.\n",
      "        성윤모 산업부 장관은 “올해 1분기에 수출 조기 플러스 전환을 목표로 총력 대응 체계를 가동할 것”이라고 말했다.\n",
      "  15  “통합은 정의(正義)이고 분열은 불의(不義)다” 황교안 자유한국당 대표는 새해 첫 일성으로 보수 통합에 대한 강한 의지를 드러냈다.\n",
      "        황 대표는 1일 서울 여의도에서 가진 기자간담회에서 “하나 된 힘으로 저들(정부·여당)의 음모를 분쇄해야 한다”며 “통합의 문을 열고 통합의 열차를 출발시키도록 하겠다.\n",
      "        자유민주세력이 ‘통합추진위원회’라는 통합 열차에 승차해달라”고 말했다.\n",
      "        그는 “과감하고 신속하게 통합을 진행하고자 한다.\n",
      "        불신과 의심을 버리고 모두 참여할 수 있도록 저는 어떤 기득권도 주장하지 않겠다”라고도 했다.\n",
      "         당내에선 “황 대표가 어느 때보다도 통합에 대한 절실함을 내비친 것 같다”(한국당 재선 의원)는 반응이 나왔다.\n",
      "        황 대표가 통합을 언급한 건 이번이 처음은 아니다.\n",
      "        그는 지난 11월 6일 “자유민주주의와 시장경제의 헌법 가치를 받드는 모든 분과의 정치적 통합을 추진하겠다”고 했고, 지난달 26일에도 병상 메시지를 통해 “흩어지고 분열해서는 저들을 막을 수 없다”고 했다.\n",
      "         하지만 적극적인 제스쳐라기보다 원론적인 수준에서만 통합을 거론한다는 비판이 당 안팎에서 나왔다.\n",
      "        한 한국당 의원은 “황 대표의 과거 발언은 한국당에 ‘넘어오라’는 고압적 뉘앙스가 있었다”고 했다.\n",
      "        황 대표의 표현대로 보수 진영에서 ‘불신과 의심’이 팽배했다는 것이다.\n",
      "        하지만 이날 황 대표 스스로 “기득권을 내려놓겠다”고 자세를 낮춤으로써 보수 통합의 의지를 드러낸 것으로 보인다.\n",
      "        달라진 황 대표의 뒤편엔 ‘총선 배수진’을 친 한국당이 있다.\n",
      "        실제 패스트트랙(신속처리안건) 정국 이후 한국당에는 “남은 건 총선뿐이다”는 위기감이 흐르고 있다.\n",
      "        당 관계자는 “의석에서 밀려 선거법도, 공수처법도 속수무책으로 내줬다.\n",
      "        숫자 싸움에서 이기려면 무조건 통합을 해야 한다는 공감대가 있다”고 분위기를 전했다.\n",
      "         유승민 새로운보수당 인재영입위원장도 이날 ‘숫자의 힘’을 얘기했다.\n",
      "        그는 국회 의원회관에 열린 신년하례회에서 “국회 안에서는 숫자의 힘이 작용하기 때문에 어떻게든 중도보수 세력이 국회 과반을 차지하는 게 중요하다”며 “아무리 늦어도 2월 초까지는 통합이든 연대든 총선에서 이길 전략을 수립해야 한다”고 했다.\n",
      "        “머릿수로 밀어붙이는 수적 열세를 극복하지 못했다”는 전날 심재철 한국당 원내대표의 발언과 일맥상통한다.\n",
      "        유 위원장은 한국당과의 통합에 대해선 “조심스럽지만 새로운보수당이 지지를 얻는다면 저희와 통합 또는 연대를 할 수밖에 없을 것”이라고 했다.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tabulate(\n",
    "        zip(\n",
    "            range(len(summaries_after_tuning)),\n",
    "            summaries_after_tuning,\n",
    "            summaries_before_tuning,\n",
    "        ),\n",
    "        headers=[\"Id\", \"Summary after\", \"Summary before\"],\n",
    "    )\n",
    ")\n",
    "print(\"\\nTarget summaries:\\n\")\n",
    "print(\n",
    "    tabulate(list(enumerate(test_samples[\"sentence_per_20\"])), headers=[\"Id\", \"Target summary\"])\n",
    ")\n",
    "print(\"\\nSource documents:\\n\")\n",
    "print(tabulate(list(enumerate(test_samples[\"input\"])), headers=[\"Id\", \"sentence_per_20\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
