{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f040e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.10.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.10.0)\n",
      "Requirement already satisfied: rouge_score in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: datasets in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.0.2)\n",
      "Requirement already satisfied: transformers==4.24.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (4.24.0)\n",
      "Requirement already satisfied: transformer-utils in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: packaging in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (21.3)\n",
      "Requirement already satisfied: wandb in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.13.5)\n",
      "Requirement already satisfied: pandas in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (1.3.5)\n",
      "Requirement already satisfied: numpy in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.21.6)\n",
      "Requirement already satisfied: matplotlib in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (3.5.3)\n",
      "Requirement already satisfied: dataclasses in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (0.6)\n",
      "Requirement already satisfied: tqdm in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (4.64.1)\n",
      "Requirement already satisfied: rouge in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from torch==1.10.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: filelock in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: requests in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (2.28.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: nltk in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: xxhash in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (10.0.1)\n",
      "Requirement already satisfied: dill in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.4)\n",
      "Requirement already satisfied: colorcet in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformer-utils->-r requirements.txt (line 5)) (3.0.1)\n",
      "Requirement already satisfied: seaborn in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformer-utils->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from packaging->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (3.1.29)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (0.4.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (8.1.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.11.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (2.3)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (4.21.9)\n",
      "Requirement already satisfied: pathtools in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.0.11)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (5.9.4)\n",
      "Requirement already satisfied: setuptools in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (65.5.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 8)) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (9.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (1.4.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 7)) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (1.26.13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyct>=0.4.4 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from colorcet->transformer-utils->-r requirements.txt (line 5)) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from importlib-metadata->transformers==4.24.0->-r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: joblib in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from nltk->rouge_score->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements.txt (line 7)) (5.0.0)\n",
      "Requirement already satisfied: param>=1.7.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pyct>=0.4.4->colorcet->transformer-utils->-r requirements.txt (line 5)) (1.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a24dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from rouge import Rouge\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    EarlyStoppingCallback,BartTokenizerFast\n",
    "\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "896ecbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = \"checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06bb2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (..) 제거\n",
    "    #sentence = re.sub(r'[#@]+[가-힣A-Za-z#]+', ' ', sentence)\n",
    "    sentence = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+[/ㄱ-ㅎㅏ-ㅣ]', '', sentence) # 여러개 자음과 모음을 삭제한다.\n",
    "    sentence = re.sub(\"[^가-힣a-z0-9#@]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "    # 스폐셜 토큰 적용할 거면 여기 위에 영어 외 문자 공백으로 만들때 스폐셜 토큰을 넘어갈 수 있도록 지정해주면된다.\n",
    "    # 그리고 세번째 정규표현식을 지워야 할 것이다. \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60899161",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_total.csv')\n",
    "val_df = pd.read_csv('data/val_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a2609b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Category = train_df['Category'].unique()\n",
    "val_Category = val_df['Category'].unique()\n",
    "def categori_ext(data, Category, tv):\n",
    "    df = pd.DataFrame()\n",
    "    for c in Category:\n",
    "        df = pd.concat([df, data[data['Category'] == c].iloc[0:int(len(data[data['Category'] == c])*0.05)]], axis = 0)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358e7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = categori_ext(train_df, train_Category, 'train')\n",
    "val_df = categori_ext(val_df, val_Category, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "331ce8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fd321028-d5b4-55f7-9e20-2eaa262f9154</td>\n",
       "      <td>['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6</td>\n",
       "      <td>['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e90e721f-00d1-5114-aa5d-5f1061472a29</td>\n",
       "      <td>['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b215f3a2-d647-59f9-8410-1274ee5edd97</td>\n",
       "      <td>['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0bda61b6-1396-5a2a-a049-0b4035e40d59</td>\n",
       "      <td>['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  fd321028-d5b4-55f7-9e20-2eaa262f9154   \n",
       "1  c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6   \n",
       "2  e90e721f-00d1-5114-aa5d-5f1061472a29   \n",
       "3  b215f3a2-d647-59f9-8410-1274ee5edd97   \n",
       "4  0bda61b6-1396-5a2a-a049-0b4035e40d59   \n",
       "\n",
       "                                                Text  \\\n",
       "0  ['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...   \n",
       "1  ['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...   \n",
       "2  ['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...   \n",
       "3  ['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...   \n",
       "4  ['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...   \n",
       "\n",
       "                                             Summary Category  \n",
       "0               비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.  상거래(쇼핑)  \n",
       "1                비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.  상거래(쇼핑)  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  상거래(쇼핑)  \n",
       "3            칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.  상거래(쇼핑)  \n",
       "4                  잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.  상거래(쇼핑)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82495de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 13994/13994 [00:00<00:00, 33610.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 13994/13994 [00:00<00:00, 81057.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "train_text = []\n",
    "train_summary = []\n",
    "\n",
    "for tt in tqdm(train_df['Text']):\n",
    "    train_text.append(preprocess_sentence(tt))\n",
    "\n",
    "for ts in tqdm(train_df['Summary']):\n",
    "      train_summary.append(preprocess_sentence(ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa9f8718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1746/1746 [00:00<00:00, 33995.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1746/1746 [00:00<00:00, 88067.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "val_text = []\n",
    "val_summary = []\n",
    "\n",
    "for vt in tqdm(val_df['Text']):\n",
    "    val_text.append(preprocess_sentence(vt))\n",
    "\n",
    "for vs in tqdm(val_df['Summary']):\n",
    "      val_summary.append(preprocess_sentence(vs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f5ad0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 13994/13994 [00:00<00:00, 2498301.28it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1746/1746 [00:00<00:00, 2310175.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "train_Category = []\n",
    "val_Category= []\n",
    "\n",
    "for tc in tqdm(train_df['Category']):\n",
    "    train_Category.append(tc)\n",
    "\n",
    "for vc in tqdm(val_df['Category']):\n",
    "    val_Category.append(vc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2cc689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_text)):\n",
    "    temp_cat = \"#\"+val_Category[i]+\"# \"\n",
    "    val_text[i] = temp_cat+val_text[i]\n",
    "for i in range(len(train_text)):\n",
    "    temp_cat = \"#\"+train_Category[i]+\"# \"\n",
    "    train_text[i] = temp_cat+train_text[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "855c6461",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(train_text,train_summary), columns=['Text', 'Summary'])\n",
    "val_df = pd.DataFrame(zip(val_text,val_summary), columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9919048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  #상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우...   \n",
       "1  #상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...   \n",
       "2  #상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...   \n",
       "3  #상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...   \n",
       "4  #상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...   \n",
       "\n",
       "                                             Summary  \n",
       "0                 비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다  \n",
       "1                 비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
       "3                칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다  \n",
       "4                   잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aae0d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_df) \n",
    "val_data = Dataset.from_pandas(val_df)\n",
    "test_samples = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "474e67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 128\n",
    "max_target = 32\n",
    "batch_size = 4\n",
    "ignore_index = -100# tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9475f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    " def add_ignored_data(inputs, max_len, ignore_index):\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51c5e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding_data(inputs, max_len):\n",
    "        pad_index = tokenizer.pad_token_id\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [pad_index] *(max_len - len(inputs))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "170b779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    label_id= []\n",
    "    label_ids = []\n",
    "    dec_input_ids = []\n",
    "    input_ids = []\n",
    "\n",
    "    for i in range(len(data_to_process['Text'])):\n",
    "        input_ids.append(add_padding_data(tokenizer.encode(data_to_process['Text'][i], add_special_tokens=False), max_input))\n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_id.append(tokenizer.encode(data_to_process['Summary'][i]))  \n",
    "        label_id[i].append(tokenizer.eos_token_id)  \n",
    "    for i in range(len(data_to_process['Summary'])):  \n",
    "        dec_input_id = [tokenizer.eos_token_id]\n",
    "        dec_input_id += label_id[i][:-1]\n",
    "        dec_input_ids.append(add_padding_data(dec_input_id, max_target))  \n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
    "   \n",
    "    return {'input_ids': input_ids,\n",
    "            'attention_mask' : (np.array(input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'decoder_input_ids': dec_input_ids,\n",
    "            'decoder_attention_mask': (np.array(dec_input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'labels': label_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0733144b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30031, 768, padding_idx=3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_words = [\n",
    "                \"#@주소#\", \"#@이모티콘#\", \"#@이름#\", \"#@URL#\", \"#@소속#\",\n",
    "                \"#@기타#\", \"#@전번#\", \"#@계정#\", \"#@url#\", \"#@번호#\", \"#@금융#\", \"#@신원#\",\n",
    "                \"#@장소#\", \"#@시스템#사진#\", \"#@시스템#동영상#\", \"#@시스템#기타#\", \"#@시스템#검색#\",\n",
    "                \"#@시스템#지도#\", \"#@시스템#삭제#\", \"#@시스템#파일#\", \"#@시스템#송금#\", \"#@시스템#\",\n",
    "                \"#개인 및 관계#\", \"#미용과 건강#\", \"#상거래(쇼핑)#\", \"#시사/교육#\", \"#식음료#\", \n",
    "                \"#여가 생활#\", \"#일과 직업#\", \"#주거와 생활#\", \"#행사#\",\n",
    "                ]\n",
    "\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_words})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6541f767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ac23235f814316bfd8b073c7d2661b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26af0c7db82414ba54c6ce37da19f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16002c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.max_length = 32 \n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "123f9a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"kobart_version\": 2.0,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 32,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 5,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30031\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c65b614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return rouge.get_scores(pred_str, label_str, avg=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7d2b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoint/mlm_test\",\n",
    "    num_train_epochs=5,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=3e-05,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end = True,\n",
    "    logging_strategy = 'epoch',\n",
    "    evaluation_strategy  = 'epoch',\n",
    "    save_strategy ='epoch'\n",
    "\n",
    "    #evaluation_strategy = \"steps\",# step별로 2버 loss가 오르는거 아니면 계속 반복하는듯\n",
    "    #load_best_model_at_end = True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7c0b86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19d2fca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "   # callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22dca426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 13994\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4375\n",
      "  Number of trainable parameters = 123883776\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jx7789/Download/koBART/wandb/run-20221211_152142-395sys1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/395sys1y\" target=\"_blank\">checkpoint/mlm_test</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4375' max='4375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4375/4375 19:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.911300</td>\n",
       "      <td>3.559593</td>\n",
       "      <td>{'r': 0.21172322395752166, 'p': 0.22621014920241997, 'f': 0.21196805910405592}</td>\n",
       "      <td>{'r': 0.07451243391473, 'p': 0.07909349020570487, 'f': 0.07399289295104912}</td>\n",
       "      <td>{'r': 0.20050646441435746, 'p': 0.2140989635463295, 'f': 0.20069120036058652}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.287400</td>\n",
       "      <td>3.386477</td>\n",
       "      <td>{'r': 0.21990579179993475, 'p': 0.23976651580535596, 'f': 0.22202442033375314}</td>\n",
       "      <td>{'r': 0.08246561677096402, 'p': 0.09011721883032793, 'f': 0.08278915937287869}</td>\n",
       "      <td>{'r': 0.20912627820265828, 'p': 0.22799624785710162, 'f': 0.21111617065995883}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.994900</td>\n",
       "      <td>3.376666</td>\n",
       "      <td>{'r': 0.23050117568397221, 'p': 0.23732729729994165, 'f': 0.22627751643558125}</td>\n",
       "      <td>{'r': 0.08652864499775974, 'p': 0.09034601769792167, 'f': 0.08488592541994192}</td>\n",
       "      <td>{'r': 0.21956683782415837, 'p': 0.22644714657553383, 'f': 0.2156864550797577}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.804200</td>\n",
       "      <td>3.386734</td>\n",
       "      <td>{'r': 0.23750545414668825, 'p': 0.23659499665605144, 'f': 0.22983471487949567}</td>\n",
       "      <td>{'r': 0.08828375133618617, 'p': 0.08868652175321295, 'f': 0.08521863295812991}</td>\n",
       "      <td>{'r': 0.22450647535297394, 'p': 0.22378888708641986, 'f': 0.21731156287675926}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.685500</td>\n",
       "      <td>3.394754</td>\n",
       "      <td>{'r': 0.2357506870038071, 'p': 0.2390430559950092, 'f': 0.23020894390313654}</td>\n",
       "      <td>{'r': 0.09030535081457772, 'p': 0.09279612894270371, 'f': 0.08820982582977725}</td>\n",
       "      <td>{'r': 0.22340803675207532, 'p': 0.22671697616862344, 'f': 0.21825673775289597}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/mlm_test/checkpoint-875\n",
      "Configuration saved in checkpoint/mlm_test/checkpoint-875/config.json\n",
      "Model weights saved in checkpoint/mlm_test/checkpoint-875/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/mlm_test/checkpoint-875/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/mlm_test/checkpoint-875/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/mlm_test/checkpoint-1750\n",
      "Configuration saved in checkpoint/mlm_test/checkpoint-1750/config.json\n",
      "Model weights saved in checkpoint/mlm_test/checkpoint-1750/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/mlm_test/checkpoint-1750/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/mlm_test/checkpoint-1750/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/mlm_test/checkpoint-2625\n",
      "Configuration saved in checkpoint/mlm_test/checkpoint-2625/config.json\n",
      "Model weights saved in checkpoint/mlm_test/checkpoint-2625/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/mlm_test/checkpoint-2625/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/mlm_test/checkpoint-2625/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/mlm_test/checkpoint-3500\n",
      "Configuration saved in checkpoint/mlm_test/checkpoint-3500/config.json\n",
      "Model weights saved in checkpoint/mlm_test/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/mlm_test/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/mlm_test/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/mlm_test/checkpoint-875] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/mlm_test/checkpoint-4375\n",
      "Configuration saved in checkpoint/mlm_test/checkpoint-4375/config.json\n",
      "Model weights saved in checkpoint/mlm_test/checkpoint-4375/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/mlm_test/checkpoint-4375/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/mlm_test/checkpoint-4375/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/mlm_test/checkpoint-1750] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoint/mlm_test/checkpoint-2625 (score: 3.376666307449341).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4375, training_loss=3.1366516183035715, metrics={'train_runtime': 1198.8185, 'train_samples_per_second': 58.366, 'train_steps_per_second': 3.649, 'total_flos': 5332907497881600.0, 'train_loss': 3.1366516183035715, 'epoch': 5.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c45f7c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.376666307449341,\n",
       " 'eval_rouge-1': {'r': 0.23050117568397221,\n",
       "  'p': 0.23732729729994165,\n",
       "  'f': 0.22627751643558125},\n",
       " 'eval_rouge-2': {'r': 0.08652864499775974,\n",
       "  'p': 0.09034601769792167,\n",
       "  'f': 0.08488592541994192},\n",
       " 'eval_rouge-l': {'r': 0.21956683782415837,\n",
       "  'p': 0.22644714657553383,\n",
       "  'f': 0.2156864550797577},\n",
       " 'eval_runtime': 57.1627,\n",
       " 'eval_samples_per_second': 30.544,\n",
       " 'eval_steps_per_second': 0.49,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "537c7188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30031\n",
      "}\n",
      "\n",
      "loading weights file checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, num_beams=5,no_repeat_ngram_size=3,\n",
    "                            attention_mask=attention_mask, \n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bbd1946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 0/1746 [00:00<?, ?it/s]/home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 32 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n",
      "100%|██████████████████████████████████████████████████████████████████████| 1746/1746 [03:49<00:00,  7.62it/s]\n"
     ]
    }
   ],
   "source": [
    "#summaries_before_tuning = []\n",
    "#for test_sample in tqdm(test_samples):\n",
    "#    summaries_before_tuning.append(generate_summary(test_sample, model_before_tuning)[1])\n",
    "#summaries_before_tuning = list(itertools.chain(*summaries_before_tuning))    \n",
    "    \n",
    "summaries_after_tuning=[]\n",
    "for test_sample in tqdm(test_samples):\n",
    "    summaries_after_tuning.append(generate_summary(test_sample, model)[1])\n",
    "summaries_after_tuning = list(itertools.chain(*summaries_after_tuning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c940fae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.21027270030216685,\n",
       "  'p': 0.24031157700679587,\n",
       "  'f': 0.2166579382917027},\n",
       " 'rouge-2': {'r': 0.07543741309149801,\n",
       "  'p': 0.0876742034590227,\n",
       "  'f': 0.07767351281260217},\n",
       " 'rouge-l': {'r': 0.2003152042747642,\n",
       "  'p': 0.22867740783649368,\n",
       "  'f': 0.20624994446349973}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(summaries_after_tuning, test_samples[\"Summary\"], avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69632a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_0 \n",
      "\n",
      "Summary after \n",
      " 영업팀 과장님이 보내줬는데 팀장님이 해줄지 모르겠다\n",
      "\n",
      "Target summary \n",
      " 팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다\n",
      "\n",
      "Text #일과 직업# 웅 영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소로 엄청 싸워서 웅 흥 4개월가는거도아니고 3일가는데 좀해주지 거 얼마한다구 내말이 아니 해봤자 3일 다 합해도 몇만원 차이인데 너무해 그때는 2일이었는데도 만원 더 싼데에서 자꾸 자라고 넘 시러 일단\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_100 \n",
      "\n",
      "Summary after \n",
      " 본사 직원이 한 달에 한 번 와서 면접 보러 간다고 한다\n",
      "\n",
      "Target summary \n",
      " 설빙의 본사 직원이 한 달에 한 번씩 오는데 한 시간 정도 이야기하고 있다\n",
      "\n",
      "Text #일과 직업# #@소속#본사직원이왓노 누가 아진짜 뭔 일이래 한달에한번씩 온대 근데 한시간 정도 말하는듯 난 면접보러온줄알앗어 오 몰랐네 무슨얘기 지\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_200 \n",
      "\n",
      "Summary after \n",
      " 이번 주 날씨가 정말 별로라고 하니 티 하나 입고 왔다고 한다\n",
      "\n",
      "Target summary \n",
      " 이번 주 날씨가 더웠다가 비 온다는데 별로다\n",
      "\n",
      "Text #주거와 생활# 근데 이번주 날씨 완전 별로더라 오늘 27도까지 그니까 올라가고 나도 티하나입고왔오 화수목 비오고 장마도 아니고 이게뭐얏 나도 가서 가디건벗을라고 더워 이제 후엉 더워더워 이제 티 한장만 입어야돼 수욜도 비오네 우리영화보는 날인데 오지말라고 기도하자\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_300 \n",
      "\n",
      "Summary after \n",
      " 빵 집 바로 옆에 카페 마가렌 양갱 파는 곳이냐고 하자 체육센터 쪽이라고 한다\n",
      "\n",
      "Target summary \n",
      " 주소로 위치를 확인하고 헬스를 두 번 하고 오라 한다\n",
      "\n",
      "Text #주거와 생활# 아냐 거기야 빵 집 바로옆 아 카페 마가렡 양갱파는곳이 아닌가 저 주소 체육센터쪽이야 이런 가서 헬스 두 번 하고 와 개멀자나\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_400 \n",
      "\n",
      "Summary after \n",
      " 온니에게 딱 맞으니 가슴이 클 수 있으니 얼른 가서 입어보라고 한다\n",
      "\n",
      "Target summary \n",
      " 옷이 클 수도 있어서 얼른 입어봤는데 지퍼 부분이 붕 떠서 싫다고 한다\n",
      "\n",
      "Text #주거와 생활# #@시스템#사진# 왔따한다 #@이름#온니 큰강 큰가요 저한테딱맞으니 가슴이 좀 클숟 수도 얼른가서 입어볼래 #@이모티콘# 근데 단점 지퍼부분 좀 붕떠 요성 상 시르다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_500 \n",
      "\n",
      "Summary after \n",
      " 저녁으로 포항 짐을 풀고 저녁을 먹기로 했다\n",
      "\n",
      "Target summary \n",
      " 저녁으로 숙소 근처에서 맛있는 데를 찾아서 대게를 먹을 것이다\n",
      "\n",
      "Text #행사# 머양 저녁은 그롬 머먹어 저녁에 대게 먹고시펑 알았쩡 그람 포항 짐풀고 저녁묵장 오라방은 맥쮸한모금만해야딩 저기 말고 딴데가면 되지 딴데도 저런덴 많앙 그냥 숙소 근처에 맛있는데 찾아서 먹자 저기 다음에 가구 알았쯍 알아볼게용\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_600 \n",
      "\n",
      "Summary after \n",
      " 사료를 주문해달라고 해서 목요일 도착 예정으로 주문했다\n",
      "\n",
      "Target summary \n",
      " 고양이 사료를 얼마나 많이 살지 대화한다\n",
      "\n",
      "Text #상거래(쇼핑)# 냥이 사료 주문해주쎄용 아예 2포하까 60키로 엥 40키로지 #@시스템#사진# 목요일 도착 예정 먼저 간 예삐가 엄마한테 매일매일 고마워할거에요 착한 #@이름# 보내준 엄마 딸도 나도 #@시스템#사진# 엄마는 왜 100 안 나옴 엄마 룰렛은 300부터 시작해요 아니 증말 자랑을 할 수가 없네 내껀 그런가봐 뿡뿡\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_700 \n",
      "\n",
      "Summary after \n",
      " 애교쟁이 마스크가 품절되기 전에 많이 사서 아에르 마스크를 주문했다\n",
      "\n",
      "Target summary \n",
      " 아직 여유 있게 주문 가능해서 아에르 마스크를 많이 사 놓으려고 한다\n",
      "\n",
      "Text #상거래(쇼핑)# 애교쟁이 마스크는 완료 역시 최고 근데 아에르 있어 응 아직은 여유있게 주문가능해 다행이야 진짜 품절되기전에 많이 사놔야지 #@시스템#사진# 역시 그럼 한달정도는 넉넉하겠다 서방님 응 앞으론 쭉 마스크는 써야될거같아\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_800 \n",
      "\n",
      "Summary after \n",
      " 유명한 웹툰 안나라 수마나라를 열심히 봤는데 언니가 좋아한다\n",
      "\n",
      "Target summary \n",
      " 언니에게 유명한 웹툰 안나라 수마나라가 드라마화된다고 하니 흑백 웹툰이었는데 드라마는 어떻게 연출했을지 기대된다고 한다\n",
      "\n",
      "Text #여가 생활# 유명한 웹툰 안나라수마나라 있잖아 어 알지 언니 그거 열심히 봤었어 당시에 그니까 연재하던 당시에 오홍 그렇구나 그럼 언니가 좋아할 수 있는 소식이야 뭔데 그래 안나라수마나라가 드라마화 된대 와 그렇구나 안나라수마나라가 흑백 웹툰이었는데 그만의 분위기를 만들어내는데 한 몫 했다고 생각하거든 드라마는 과연 어떻게 연출했을지 기대되네\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_900 \n",
      "\n",
      "Summary after \n",
      " 코스트코가 최고인정을 받고 학요근처에 코코가 없어서 아쉽다고 한다\n",
      "\n",
      "Target summary \n",
      " 코스트코가 최고인데 학교 근처에 없어서 아쉽다\n",
      "\n",
      "Text #여가 생활# 역시 코코가 최고 인정인정 학요근처에 코코가없어 학교 앗 너무 아쉽다 그니까하 말두안대\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1000 \n",
      "\n",
      "Summary after \n",
      " 머리카락 염색한 걸 못 봐서 아쉬워했더니 카카오톡 프로필 사진을 보내라고 한다\n",
      "\n",
      "Target summary \n",
      " 머리 염색한 사진을 카카오톡 프로필 사진으로 확인하라고 한다\n",
      "\n",
      "Text #미용과 건강# 머리카락 염색한걸 못 봐서 아쉬워했더니 카톡프사보래 쌀알만한크기로나왔으면서 쌀알만한크기 염색 무슨색으로 다크브라운 내가 어두운색이 더 잘어울릴거라고 했거든 오오\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1100 \n",
      "\n",
      "Summary after \n",
      " 전공만 알면 되는데 다른 전공은 읽어 보기만 해도 될 것 같다고 한다\n",
      "\n",
      "Target summary \n",
      " 전공만 알면 되고 다른 것은 읽어 보기만 해도 될 것 같다지만 그래도 한 번은 읽고 가야 할 것 같다\n",
      "\n",
      "Text #시사/교육# 전공만 알면되게찌 열공해랑 진짜 다른거는 읽어보기만 해도 될꺼야 라며 위안둥 중 고맙다잉 그랴 일회독은 하고 가여지 적어도\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1200 \n",
      "\n",
      "Summary after \n",
      " 오늘 별로 돌아다니지도 않았는데 3번이나 아가씨 하면서 말을 걸었다\n",
      "\n",
      "Target summary \n",
      " 이상한 아저씨가 말을 걸면 안 보이는 척하고 앞만 보고 가야 한다\n",
      "\n",
      "Text #개인 및 관계# 나 오늘 별로 돌아다니지도않았눈데 그 이단같은 사람들있잖아 3번이나 아가씨 하면서말걸고 어떤사람은 방금 하나은행 어딨냐고 이상한 아저씨가 물어봄 너가 말 잘듣게 생겼나바 나도 많이 그럼 얼른지나가 앜 그런가봐 그래서 안보이는척하고 앞만보고갔어 근데 진짜필요함 안하면 피곤해져 맞아 그럼 말안걸엉 대꾸안해줘야됨\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1300 \n",
      "\n",
      "Summary after \n",
      " 15분 정도 똥을 쌌는데 왜이리 격하게 반응했냐고 한다\n",
      "\n",
      "Target summary \n",
      " 똥을 싼 사람이 오랫동안 못 쌌다는 사람에게 차전자피를 추천해 준다\n",
      "\n",
      "Text #개인 및 관계# 아 망했다 똥매려 미친 아직 15분정도 가여되는데 아나 똥 쌌니 #@이름# 왜이리 격하게 반응해줘 쌌습니다 똥루 개이덕 시 끄럽게하기 아 똥못싼지 개오래됐누흑흑 엥 그렇다면 차전자피를 추천해줄게\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1400 \n",
      "\n",
      "Summary after \n",
      " 옆집 꽃순이만 맨날 구경하고 있다\n",
      "\n",
      "Target summary \n",
      " 옆집 꽃순이가 누군지 사진으로 보여주고 수달 사는 것도 아는지 이야기하자 구미가 생태 도시라고 말한다\n",
      "\n",
      "Text #개인 및 관계# 나도 키울 여건도 안되고 옆집 꽃순이만 맨날 구경한다 꽃순이가 누군데 #@시스템#사진# 아까도 봤지 꽃순 야너 금오산 저수지에 수달 사는거 아나 어니 생태도시네 구미\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1500 \n",
      "\n",
      "Summary after \n",
      " 귀엽지만 가까이가면 쳐 맞을 것 같으니 목욕시키고 싶다\n",
      "\n",
      "Target summary \n",
      " 뚱해서 날카롭게 경계하는 눈빛이 귀엽다고 꼬질꼬질한 모습이 목욕을 시키고 싶다고 한다\n",
      "\n",
      "Text #개인 및 관계# 뭔가 뚱해서 귀엽다 경계오지는 눈빛 긍게 가까이가면 쳐 맞겠다 때꾸정물봐 목욕시키고 싶다 뒤지게맞을듯 눈빛이 아주 날카로와 귀엽다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1600 \n",
      "\n",
      "Summary after \n",
      " 회가 먹고 싶은데 참치가 안 맞아서 긍정적으로 생각해 먹자고 한다\n",
      "\n",
      "Target summary \n",
      " 참치 회를 김과 같이 먹으면 맛있지만 참치가 안 맞아서 광어와 연어를 먹고 싶다고 한다\n",
      "\n",
      "Text #식음료# 나 요즘 회 먹고 싶어 어떻게 생각해 긍정적으로 생각해 먹자 흠 두툼하게 땡김 참치 아님 연어 어떰 나 참치가 안 맞아 광어에 연어 각임 참치 맛있는데 김이랑 같이 먹으면 크으 입에서 걍 녹지유\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1700 \n",
      "\n",
      "Summary after \n",
      " 설빙에 가서 빙수를 퍼먹고 싶다고 하니 이번엔 무슨 빙수가 먹고 싶은데 특별하다고 한다\n",
      "\n",
      "Target summary \n",
      " 설빙에 가서 구슬 아이스크림 설빙을 퍼먹고 싶다고 말한다\n",
      "\n",
      "Text #식음료# 설빙가서 빙수 퍼먹고싶다아 아앙 이번엔 무슨 빙수가 드시고 싶은데 좀 특별해유 특별 구슬아이스크림 설빙 그렇구나 우히히히히\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(summaries_after_tuning), 100):\n",
    "    print('idx_{} '.format(i))\n",
    "    #print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d4257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
