{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b22488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.10.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.10.0)\n",
      "Requirement already satisfied: rouge_score in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: datasets in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.0.2)\n",
      "Requirement already satisfied: transformers==4.24.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (4.24.0)\n",
      "Requirement already satisfied: transformer-utils in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: packaging in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (21.3)\n",
      "Requirement already satisfied: wandb in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.13.5)\n",
      "Requirement already satisfied: pandas in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (1.3.5)\n",
      "Requirement already satisfied: numpy in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.21.6)\n",
      "Requirement already satisfied: matplotlib in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (3.5.3)\n",
      "Requirement already satisfied: dataclasses in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (0.6)\n",
      "Requirement already satisfied: tqdm in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (4.64.1)\n",
      "Requirement already satisfied: rouge in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from torch==1.10.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: filelock in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (2022.10.31)\n",
      "Requirement already satisfied: importlib-metadata in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: requests in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (0.13.2)\n",
      "Requirement already satisfied: nltk in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: xxhash in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (10.0.1)\n",
      "Requirement already satisfied: dill in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.4)\n",
      "Requirement already satisfied: colorcet in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformer-utils->-r requirements.txt (line 5)) (3.0.1)\n",
      "Requirement already satisfied: seaborn in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformer-utils->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from packaging->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: pathtools in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (8.1.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (3.1.29)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (2.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (65.5.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.0.11)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (4.21.9)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.11.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (5.9.4)\n",
      "Requirement already satisfied: setproctitle in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 8)) (2022.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (9.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (4.38.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 7)) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (2022.9.24)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from colorcet->transformer-utils->-r requirements.txt (line 5)) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from importlib-metadata->transformers==4.24.0->-r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: joblib in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from nltk->rouge_score->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements.txt (line 7)) (5.0.0)\n",
      "Requirement already satisfied: param>=1.7.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pyct>=0.4.4->colorcet->transformer-utils->-r requirements.txt (line 5)) (1.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eea081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "from rouge import Rouge\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    EarlyStoppingCallback\n",
    "\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e55c331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_checkpoints = \"gogamza/kobart-base-v2\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23798e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_total.csv')\n",
    "val_df = pd.read_csv('data/val_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca43ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Category = train_df['Category'].unique()\n",
    "val_Category = val_df['Category'].unique()\n",
    "def categori_ext(data, Category, tv):\n",
    "    df = pd.DataFrame()\n",
    "    for c in Category:\n",
    "        df = pd.concat([df, data[data['Category'] == c].iloc[0:int(len(data[data['Category'] == c])*0.05)]], axis = 0)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db935b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = categori_ext(train_df, train_Category, 'train')\n",
    "val_df = categori_ext(val_df, val_Category, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ee972c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fd321028-d5b4-55f7-9e20-2eaa262f9154</td>\n",
       "      <td>['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6</td>\n",
       "      <td>['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e90e721f-00d1-5114-aa5d-5f1061472a29</td>\n",
       "      <td>['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b215f3a2-d647-59f9-8410-1274ee5edd97</td>\n",
       "      <td>['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0bda61b6-1396-5a2a-a049-0b4035e40d59</td>\n",
       "      <td>['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  fd321028-d5b4-55f7-9e20-2eaa262f9154   \n",
       "1  c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6   \n",
       "2  e90e721f-00d1-5114-aa5d-5f1061472a29   \n",
       "3  b215f3a2-d647-59f9-8410-1274ee5edd97   \n",
       "4  0bda61b6-1396-5a2a-a049-0b4035e40d59   \n",
       "\n",
       "                                                Text  \\\n",
       "0  ['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...   \n",
       "1  ['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...   \n",
       "2  ['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...   \n",
       "3  ['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...   \n",
       "4  ['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...   \n",
       "\n",
       "                                             Summary Category  \n",
       "0               비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.  상거래(쇼핑)  \n",
       "1                비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.  상거래(쇼핑)  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  상거래(쇼핑)  \n",
       "3            칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.  상거래(쇼핑)  \n",
       "4                  잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.  상거래(쇼핑)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21641c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_df) \n",
    "val_data = Dataset.from_pandas(val_df)\n",
    "test_samples = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c20212eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'Id': Value(dtype='string', id=None), 'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None), 'Category': Value(dtype='string', id=None)}, num_rows: 13994)\n",
      "Dataset(features: {'Id': Value(dtype='string', id=None), 'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None), 'Category': Value(dtype='string', id=None)}, num_rows: 1746)\n",
      "Dataset(features: {'Id': Value(dtype='string', id=None), 'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None), 'Category': Value(dtype='string', id=None)}, num_rows: 1746)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e38b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 128\n",
    "max_target = 32\n",
    "batch_size = 4\n",
    "ignore_index = -100# tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e90d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " def add_ignored_data(inputs, max_len, ignore_index):\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8cfb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding_data(inputs, max_len):\n",
    "        pad_index = tokenizer.pad_token_id\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [pad_index] *(max_len - len(inputs))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "027d5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    label_id= []\n",
    "    label_ids = []\n",
    "    dec_input_ids = []\n",
    "    input_ids = []\n",
    "\n",
    "    for i in range(len(data_to_process['Text'])):\n",
    "        input_ids.append(add_padding_data(tokenizer.encode(data_to_process['Text'][i]), max_input))\n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_id.append(tokenizer.encode(data_to_process['Summary'][i]))  \n",
    "        label_id[i].append(tokenizer.eos_token_id)  \n",
    "    for i in range(len(data_to_process['Summary'])):  \n",
    "        dec_input_id = [tokenizer.eos_token_id]\n",
    "        dec_input_id += label_id[i][:-1]\n",
    "        dec_input_ids.append(add_padding_data(dec_input_id, max_target))  \n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
    "   \n",
    "    return {'input_ids': input_ids,\n",
    "            'attention_mask' : (np.array(input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'decoder_input_ids': dec_input_ids,\n",
    "            'decoder_attention_mask': (np.array(dec_input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'labels': label_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd079953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcd7412b79e481a876ee95ee7752e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009b81a1883c4420bdb4f15eb71f4c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0849ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "                              \n",
    "model.config.max_length = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcfb03c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"kobart_version\": 2.0,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"max_length\": 32,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9e06289",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return rouge.get_scores(pred_str, label_str, avg=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77a86749",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoint/non_test\",\n",
    "    num_train_epochs=5,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=3e-05,\n",
    "    weight_decay=0.1,\n",
    "    #label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end = True,\n",
    "    logging_strategy = 'epoch',\n",
    "    evaluation_strategy  = 'epoch',\n",
    "    save_strategy ='epoch'\n",
    "\n",
    "    #evaluation_strategy = \"steps\",# step별로 2버 loss가 오르는거 아니면 계속 반복하는듯\n",
    "    #load_best_model_at_end = True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb5def82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e017efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "   # callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca31c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Category, Id. If Category, Id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 13994\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4375\n",
      "  Number of trainable parameters = 123859968\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jx7789/Download/koBART/wandb/run-20221211_111736-3c1w6voq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/3c1w6voq\" target=\"_blank\">checkpoint/non_test</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4375' max='4375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4375/4375 12:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.697300</td>\n",
       "      <td>2.342048</td>\n",
       "      <td>{'r': 0.19840706126435292, 'p': 0.22694401402323766, 'f': 0.20437299538491316}</td>\n",
       "      <td>{'r': 0.06991126206506626, 'p': 0.07785951310595449, 'f': 0.07078999370548786}</td>\n",
       "      <td>{'r': 0.18964592304590125, 'p': 0.21726729980841952, 'f': 0.19550008875295527}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.104800</td>\n",
       "      <td>2.261964</td>\n",
       "      <td>{'r': 0.19502233345372225, 'p': 0.2265323612999107, 'f': 0.20192089948152125}</td>\n",
       "      <td>{'r': 0.0695220969573183, 'p': 0.08015975768986641, 'f': 0.07114365102131166}</td>\n",
       "      <td>{'r': 0.18699928827144313, 'p': 0.21759927183283218, 'f': 0.19376373059643168}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.788100</td>\n",
       "      <td>2.282877</td>\n",
       "      <td>{'r': 0.20189621693036758, 'p': 0.22437475761473286, 'f': 0.20496601672820006}</td>\n",
       "      <td>{'r': 0.0692114463454485, 'p': 0.07547736789176328, 'f': 0.06912914292706462}</td>\n",
       "      <td>{'r': 0.192893822467952, 'p': 0.21439881675072064, 'f': 0.1958636340431713}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.566200</td>\n",
       "      <td>2.335724</td>\n",
       "      <td>{'r': 0.20898850713873654, 'p': 0.22268402339837196, 'f': 0.20848514930057896}</td>\n",
       "      <td>{'r': 0.07432905977365419, 'p': 0.07842693888376824, 'f': 0.0732865911062758}</td>\n",
       "      <td>{'r': 0.1995986076637956, 'p': 0.21289631534875877, 'f': 0.19917819455647745}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.422800</td>\n",
       "      <td>2.374117</td>\n",
       "      <td>{'r': 0.20655992256876984, 'p': 0.2241043053629495, 'f': 0.20757055634802182}</td>\n",
       "      <td>{'r': 0.0720739280299568, 'p': 0.07805842761045556, 'f': 0.07189540444487122}</td>\n",
       "      <td>{'r': 0.19610765107458542, 'p': 0.21325268370300976, 'f': 0.19726629868274548}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Category, Id. If Category, Id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_test/checkpoint-875\n",
      "Configuration saved in checkpoint/non_test/checkpoint-875/config.json\n",
      "Model weights saved in checkpoint/non_test/checkpoint-875/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_test/checkpoint-875/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_test/checkpoint-875/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Category, Id. If Category, Id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_test/checkpoint-1750\n",
      "Configuration saved in checkpoint/non_test/checkpoint-1750/config.json\n",
      "Model weights saved in checkpoint/non_test/checkpoint-1750/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_test/checkpoint-1750/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_test/checkpoint-1750/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Category, Id. If Category, Id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_test/checkpoint-2625\n",
      "Configuration saved in checkpoint/non_test/checkpoint-2625/config.json\n",
      "Model weights saved in checkpoint/non_test/checkpoint-2625/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_test/checkpoint-2625/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_test/checkpoint-2625/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Category, Id. If Category, Id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_test/checkpoint-3500\n",
      "Configuration saved in checkpoint/non_test/checkpoint-3500/config.json\n",
      "Model weights saved in checkpoint/non_test/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_test/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_test/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_test/checkpoint-875] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Category, Id. If Category, Id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_test/checkpoint-4375\n",
      "Configuration saved in checkpoint/non_test/checkpoint-4375/config.json\n",
      "Model weights saved in checkpoint/non_test/checkpoint-4375/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_test/checkpoint-4375/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_test/checkpoint-4375/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_test/checkpoint-2625] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoint/non_test/checkpoint-1750 (score: 2.2619640827178955).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4375, training_loss=1.9158431640625, metrics={'train_runtime': 777.4933, 'train_samples_per_second': 89.994, 'train_steps_per_second': 5.627, 'total_flos': 5332907497881600.0, 'train_loss': 1.9158431640625, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fbcd1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Category, Id. If Category, Id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.2619640827178955,\n",
       " 'eval_rouge-1': {'r': 0.19502233345372225,\n",
       "  'p': 0.2265323612999107,\n",
       "  'f': 0.20192089948152125},\n",
       " 'eval_rouge-2': {'r': 0.0695220969573183,\n",
       "  'p': 0.08015975768986641,\n",
       "  'f': 0.07114365102131166},\n",
       " 'eval_rouge-l': {'r': 0.18699928827144313,\n",
       "  'p': 0.21759927183283218,\n",
       "  'f': 0.19376373059643168},\n",
       " 'eval_runtime': 10.4686,\n",
       " 'eval_samples_per_second': 166.785,\n",
       " 'eval_steps_per_second': 2.675,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12e1007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jx7789/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/f9f2ec35d3c32a1ecc7a3281f9626b7ec1913fed/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jx7789/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/f9f2ec35d3c32a1ecc7a3281f9626b7ec1913fed/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at gogamza/kobart-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, \n",
    "                            attention_mask=attention_mask, \n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef4bef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 0/1746 [00:00<?, ?it/s]/home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 32 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n",
      "100%|██████████████████████████████████████████████████████████████████████| 1746/1746 [02:32<00:00, 11.47it/s]\n"
     ]
    }
   ],
   "source": [
    "#summaries_before_tuning = []\n",
    "#for test_sample in tqdm(test_samples):\n",
    "#    summaries_before_tuning.append(generate_summary(test_sample, model_before_tuning)[1])\n",
    "#summaries_before_tuning = list(itertools.chain(*summaries_before_tuning))    \n",
    "    \n",
    "summaries_after_tuning=[]\n",
    "for test_sample in tqdm(test_samples):\n",
    "    summaries_after_tuning.append(generate_summary(test_sample, model)[1])\n",
    "summaries_after_tuning = list(itertools.chain(*summaries_after_tuning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "badbf1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.16535943902356573,\n",
       "  'p': 0.22122823332773803,\n",
       "  'f': 0.18195143432015265},\n",
       " 'rouge-2': {'r': 0.05618009955804081,\n",
       "  'p': 0.07258321375310152,\n",
       "  'f': 0.06027034023538989},\n",
       " 'rouge-l': {'r': 0.15944990608462628,\n",
       "  'p': 0.21419170758416758,\n",
       "  'f': 0.17576656126376963}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(summaries_after_tuning, test_samples[\"Summary\"], avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3ceeb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_0 \n",
      "\n",
      "Summary after \n",
      " 영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다.\n",
      "\n",
      "Target summary \n",
      " 팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다.\n",
      "\n",
      "Text ['웅', '영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소로 엄청 싸워서', '웅 흥 4개월가는거도아니고 3일가는데 좀해주지 거 얼마한다구', '내말이... 아니 해봤자 3일 다 합해도 몇만원 차이인데 너무해 그때는 2일이었는데도 만원 더 싼데에서 자꾸 자라고... 넘 시러..', '일단']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_100 \n",
      "\n",
      "Summary after \n",
      " 한 달 한 달 동안 본사에서 근무하는 직원이 왔다고 한다.\n",
      "\n",
      "Target summary \n",
      " 설빙의 본사 직원이 한 달에 한 번씩 오는데 한 시간 정도 이야기하고 있다.\n",
      "\n",
      "Text ['#@소속#본사직원이왓노..', '누가.! 아진짜??? 뭔ㄴ일이래', '한달에한번씩 온대. ㅋ 근데 한시간 정도 말하는듯. 난 면접보러온줄알앗어 ㅋ', '오 몰랐네.. 무슨얘기ㅘ지']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_200 \n",
      "\n",
      "Summary after \n",
      " 이번 주 날씨에 대해서 이야기하고 있다.\n",
      "\n",
      "Target summary \n",
      " 이번 주 날씨가 더웠다가 비 온다는데 별로다.\n",
      "\n",
      "Text ['근데 이번주 날씨 완전 별로더라 오늘 27도까지', '그니까..', '올라가고', '나도 티하나입고왔오', '화수목 비오고', '장마도 아니고 이게뭐얏!', '나도 가서 가디건벗을라고 더워 이제 ㅡㅡ', '후엉.......더워더워 이제 티 한장만 입어야돼', '수욜도 비오네... 우리영화보는 날인데...', '오지말라고 기도하자..!']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_300 \n",
      "\n",
      "Summary after \n",
      " 빵 집 바로 옆에 카페 마가렌이 있는데 체육센터 쪽이다.\n",
      "\n",
      "Target summary \n",
      " 주소로 위치를 확인하고 헬스를 두 번 하고 오라 한다.\n",
      "\n",
      "Text ['아냐 거기야 빵 집 바로옆 아 카페 마가렡 양갱파는곳이 아닌가', '저 주소 체육센터쪽이야', '이런ㅋㅋㅋㅋㅋ', 'ㅋㅋㅋㅋ가서 헬스 두 번 하고 와^_^', '개멀자나']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_400 \n",
      "\n",
      "Summary after \n",
      " 온다고 큰 강이 온다고 한다.\n",
      "\n",
      "Target summary \n",
      " 옷이 클 수도 있어서 얼른 입어봤는데 지퍼 부분이 붕 떠서 싫다고 한다.\n",
      "\n",
      "Text ['#@시스템#사진# 왔따한다', '#@이름#온니 큰강ㅎ 큰가요', '저한테딱맞으니 가슴이 좀 클숟ㅎ 수도', '얼른가서 입어볼래 #@이모티콘#', 'ㅋㅋㅋ근데 단점 지퍼부분', 'ㅇㅇ', '좀 붕떠 요성', 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ', '상', '시르다']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_500 \n",
      "\n",
      "Summary after \n",
      " 저녁에 저녁에 대게 먹고 싶다고 한다.\n",
      "\n",
      "Target summary \n",
      " 저녁으로 숙소 근처에서 맛있는 데를 찾아서 대게를 먹을 것이다.\n",
      "\n",
      "Text ['머양 저녁은 그롬 머먹어 !!! 저녁에 대게 먹고시펑!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '알았쩡 그람 포항 짐풀고 저녁묵장 오라방은 맥쮸한모금만해야딩', '저기 말고 딴데가면 되지 딴데도 저런덴 많앙 그냥 숙소 근처에 맛있는데 찾아서 먹자 저기 다음에 가구', '알았쯍 알아볼게용']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_600 \n",
      "\n",
      "Summary after \n",
      " 사료를 주문해달라고 해서 40킬로그램(kg)짜리 사료를 주문해달라고 한다.\n",
      "\n",
      "Target summary \n",
      " 고양이 사료를 얼마나 많이 살지 대화한다.\n",
      "\n",
      "Text ['냥이 사료 주문해주쎄용 아예 2포하까?', '60키로?', '엥? 40키로지 ㅋㅋ', '#@시스템#사진# 목요일 도착 예정 먼저 간 예삐가 엄마한테 매일매일 고마워할거에요ㅠㅠ 착한 #@이름#💕', 'ㅎㅎㅎ 보내준 엄마 딸도~~~ ㅋㄱ 나도~ #@시스템#사진#', '엄마는 왜 100 안 나옴? 엄마 룰렛은 300부터 시작해요?ㅎㅎㅎ 아니 증말 자랑을 할 수가 없네ㅋㅋㅋㅋㅋ', '내껀 그런가봐. ㅎㅎ', '뿡뿡💨']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_700 \n",
      "\n",
      "Summary after \n",
      " 마스크는 완료되었고 아에르는 아직은 여유있게 주문 가능하다.\n",
      "\n",
      "Target summary \n",
      " 아직 여유 있게 주문 가능해서 아에르 마스크를 많이 사 놓으려고 한다.\n",
      "\n",
      "Text ['애교쟁이~ 마스크는 완료!', '역시 최고! 근데 아에르 있어?', '응 아직은 여유있게 주문가능해', '다행이야 진짜', '품절되기전에 많이 사놔야지', '#@시스템#사진# 역시! 그럼 한달정도는 넉넉하겠다 서방님', '응 앞으론 쭉 마스크는 써야될거같아']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_800 \n",
      "\n",
      "Summary after \n",
      " 유명한 웹툰 안나라 수마나라를 열심히 봤다.\n",
      "\n",
      "Target summary \n",
      " 언니에게 유명한 웹툰 안나라 수마나라가 드라마화된다고 하니 흑백 웹툰이었는데 드라마는 어떻게 연출했을지 기대된다고 한다.\n",
      "\n",
      "Text ['유명한 웹툰 안나라수마나라 있잖아', '어 알지! 언니 그거 열심히 봤었어 당시에ㅋㅋㅋ 그니까 연재하던 당시에', '오홍 그렇구나!! 그럼 언니가 좋아할 수 있는 소식이야', '뭔데 그래~?', '안나라수마나라가 드라마화 된대😁', '와 그렇구나 안나라수마나라가 흑백 웹툰이었는데 그만의 분위기를 만들어내는데 한 몫 했다고 생각하거든 드라마는 과연 어떻게 연출했을지 기대되네!']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_900 \n",
      "\n",
      "Summary after \n",
      " 학요 가까운 코코가 없어서 학교 근처에 코코가 없다고 한다.\n",
      "\n",
      "Target summary \n",
      " 코스트코(코코)가 최고인데 학교 근처에 없어서 아쉽다.\n",
      "\n",
      "Text ['역시 코코가 최고~~~~', '인정인정 ㅜㅜ 학요근처에 코코가없어 학교', '앗 너무 아쉽다ㅠㅠㅠㅠㅠㅠㅠ', '그니까ㅜㅜㅜㅜ하 말두안대']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1000 \n",
      "\n",
      "Summary after \n",
      " 염색한 것을 못 봐서 아쉬워했더니 카톡프 사보래가 쌀알만한 크기로 나왔다고 한다.\n",
      "\n",
      "Target summary \n",
      " 머리 염색한 사진을 카카오톡 프로필 사진으로 확인하라고 한다.\n",
      "\n",
      "Text ['머리카락 염색한걸 못 봐서 아쉬워했더니 카톡프사보래 쌀알만한크기로나왔으면서', 'ㅋㅋㅋㅋㅋㅋㅋㅋ쌀알만한크기 염색 무슨색으로?', '다크브라운 내가 어두운색이 더 잘어울릴거라고 했거든', '오오']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1100 \n",
      "\n",
      "Summary after \n",
      " 다른 것은는 읽어보기만 해도 될 거라며 위안둥 중이라고 한다.\n",
      "\n",
      "Target summary \n",
      " 전공만 알면 되고 다른 것은 읽어 보기만 해도 될 것 같다지만 그래도 한 번은 읽고(일회독하고) 가야 할 것 같다.\n",
      "\n",
      "Text ['전공만 알면되게찌', '열공해랑 진짜', '다른거는 읽어보기만 해도 될꺼야 라며 위안둥 중 고맙다잉', '그랴 일회독은 하고 가여지 적어도']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1200 \n",
      "\n",
      "Summary after \n",
      " 오늘 별로 별로 돌아다니지도 않았는데 이단 같은 사람들이 있어서 3번이나 아가씨 하면서 말 걸었다.\n",
      "\n",
      "Target summary \n",
      " 이상한 아저씨가 말을 걸면 안 보이는 척하고 앞만 보고 가야 한다.\n",
      "\n",
      "Text ['나 오늘 별로 돌아다니지도않았눈데 그 이단같은 사람들있잖아 3번이나 아가씨 하면서말걸고 어떤사람은 방금 하나은행 어딨냐고 이상한 아저씨가 물어봄...ㅋ', '너가 말 잘듣게 생겼나바... 나도 많이 그럼...ㅠ 얼른지나가', '앜ㅋㅋㅋㅋ 그런가봐ㅜㅠ 그래서 안보이는척하고 앞만보고갔어', '근데 진짜필요함 안하면 피곤해져', 'ㅋㅋㅋㅋㅋ맞아 그럼 말안걸엉 대꾸안해줘야됨 ㅡㅡㅋ']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1300 \n",
      "\n",
      "Summary after \n",
      " 망했다고 똥을 매는 미친이 아직 15분 정도 남아서 아직 15분 정도 남은 것 같다.\n",
      "\n",
      "Target summary \n",
      " 똥을 싼 사람이 오랫동안 못 쌌다는 사람에게 차전자피를 추천해 준다.\n",
      "\n",
      "Text ['아 망했다 똥매려 미친 아직 15분정도ㅠ가여되는데', 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 아나 똥 쌌니?!!!!!!! #@이름#?!!!!!!!!!!!!!!!!!!', 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 왜이리 격하게 반응해줘..ㅎㅎㅎ 쌌습니다~ 똥루~~~~! 개이덕', 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 시ㅡ끄럽게하기 ㅎ 아 똥못싼지 개오래됐누ㅜㅜ흑흑', '엥!! 그렇다면 차전자피를 추천해줄게']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1400 \n",
      "\n",
      "Summary after \n",
      " 옆집 꽃순이만 맨날 구경한다고 한다.\n",
      "\n",
      "Target summary \n",
      " 옆집 꽃순이가 누군지 사진으로 보여주고 수달 사는 것도 아는지 이야기하자 구미가 생태 도시라고 말한다.\n",
      "\n",
      "Text ['ㅇㅇ 나도 .. 키울 여건도 안되고 .. 옆집 꽃순이만 맨날 구경한다', '꽃순이가 누군데', '#@시스템#사진# 아까도 봤지 꽃순 야너 금오산 저수지에 수달 사는거 아나', '어니 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 생태도시네 구미']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1500 \n",
      "\n",
      "Summary after \n",
      " 뚱해서 귀엽고 가까이가면 쳐 맞겠다고 할 때 가까이가면 쳐 맞겠다고 할 때 쳐 맞을 수 있다.\n",
      "\n",
      "Target summary \n",
      " 뚱해서 날카롭게 경계하는 눈빛이 귀엽다고 꼬질꼬질한 모습이 목욕을 시키고 싶다고 한다.\n",
      "\n",
      "Text ['뭔가 뚱해서 귀엽다', '경계오지는 눈빛', '긍게 ㅋㅋㅋ 가까이가면 쳐 맞겠다 때꾸정물봐 목욕시키고 싶다', 'ㅋㅋㅋㅋㅋㅋㅋ 뒤지게맞을듯', 'ㅋㅋㅋㅋㅋ 눈빛이 아주 날카로와 귀엽다']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1600 \n",
      "\n",
      "Summary after \n",
      " 요즘 회 회를 먹고 싶어 두툼하게 땡김을 먹자고 한다.\n",
      "\n",
      "Target summary \n",
      " 참치 회를 김과 같이 먹으면 맛있지만 참치가 안 맞아서 광어와 연어를 먹고 싶다고 한다.\n",
      "\n",
      "Text ['나 요즘 회 먹고 싶어 어떻게 생각해', '긍정적으로 생각해! 먹자!', '흠 두툼하게 땡김', '참치 아님 연어 어떰', '나 참치가 안 맞아 광어에 연어 각임', '참치 맛있는데 김이랑 같이 먹으면 크으 입에서 걍 녹지유?']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1700 \n",
      "\n",
      "Summary after \n",
      " 설빙 가서 빙수를 퍼 먹고 싶다고 한다.\n",
      "\n",
      "Target summary \n",
      " 설빙에 가서 구슬 아이스크림 설빙을 퍼먹고 싶다고 말한다.\n",
      "\n",
      "Text ['설빙가서 빙수 퍼먹고싶다아ㅏ아앙', '이번엔 무슨 빙수가 드시고 싶은데?', '좀 특별해유', '특별?!', '구슬아이스크림 설빙 ㅎㅅ', 'ㅋㅋㅋㅋ그렇구나', '우히히히히']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(summaries_after_tuning), 100):\n",
    "    print('idx_{} '.format(i))\n",
    "    #print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d163a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
