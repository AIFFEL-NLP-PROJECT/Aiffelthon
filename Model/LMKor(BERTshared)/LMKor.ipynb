{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1051c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c2ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('~/aiffel/aiffelthon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08898c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52565a51",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.21.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=4976236e56a9496b7b4a3bb94cfafe11b711c1e9a80210b1ff404b856722dbf6\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting datasets==1.0.2\n",
      "  Downloading datasets-1.0.2-py3-none-any.whl (1.8 MB)\n",
      "     |████████████████████████████████| 1.8 MB 6.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.21.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.26.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (0.3.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.0.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.3.3)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (4.62.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.16.0)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.14.0\n",
      "    Uninstalling datasets-1.14.0:\n",
      "      Successfully uninstalled datasets-1.14.0\n",
      "Successfully installed datasets-1.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting transformers==4.24.0\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "     |████████████████████████████████| 5.5 MB 6.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2021.11.10)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "     |████████████████████████████████| 7.6 MB 68.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (1.21.4)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "     |████████████████████████████████| 163 kB 101.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.0.8)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.19\n",
      "    Uninstalling huggingface-hub-0.0.19:\n",
      "      Successfully uninstalled huggingface-hub-0.0.19\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.11.3\n",
      "    Uninstalling transformers-4.11.3:\n",
      "      Successfully uninstalled transformers-4.11.3\n",
      "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting transformer-utils\n",
      "  Downloading transformer_utils-0.1.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (1.9.1+cu111)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.24.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.62.3)\n",
      "Collecting colorcet\n",
      "  Downloading colorcet-3.0.1-py2.py3-none-any.whl (1.7 MB)\n",
      "     |████████████████████████████████| 1.7 MB 6.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: seaborn in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (0.11.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyct>=0.4.4\n",
      "  Downloading pyct-0.4.8-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.7.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (3.4.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->transformer-utils) (4.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.13.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (3.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (8.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (3.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->seaborn->transformer-utils) (2021.3)\n",
      "Collecting param>=1.7.0\n",
      "  Downloading param-1.12.2-py2.py3-none-any.whl (86 kB)\n",
      "     |████████████████████████████████| 86 kB 10.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn->transformer-utils) (1.16.0)\n",
      "Installing collected packages: param, pyct, colorcet, transformer-utils\n",
      "Successfully installed colorcet-3.0.1 param-1.12.2 pyct-0.4.8 transformer-utils-0.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "!pip install datasets==1.0.2\n",
    "!pip install transformers==4.24.0\n",
    "!pip install transformer-utils\n",
    "!pip install packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd188f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#Tokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "#Encoder-Decoder Model\n",
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "#Tokenizer, Encoder-Decoder Model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "#Training\n",
    "from seq2seq_trainer import Seq2SeqTrainer\n",
    "from transformers import TrainingArguments\n",
    "from seq2seq_training_args import Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf65fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 지정\n",
    "#%cd ~/aiffel/aiffelthon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip ~/aiffel/aiffelthon/csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0971c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/legacy/seq2seq/seq2seq_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07565f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/huggingface/transformers/blob/main/src/transformers/utils/import_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9162f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/configuration_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d03a7be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125690 18300\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "train_df = pd.read_csv('/aiffel/aiffel/aiffelthon/csv/train_Sum1.csv')\n",
    "#train_df.drop(labels = 'Unnamed: 0', axis = 1, inplace = True)\n",
    "train_df.rename(columns = {\"input\": \"Text\"}, inplace = True)\n",
    "train_df.rename(columns = {\"sentence_onesent\": \"Summary\"}, inplace = True)\n",
    "#train_df.rename(columns = {\"sentence_per_20\": \"Summary\"}, inplace = True)\n",
    "\n",
    "val_df = pd.read_csv('/aiffel/aiffel/aiffelthon/csv/val_Sum1.csv')\n",
    "#val_df.drop(labels = 'Unnamed: 0', axis = 1, inplace = True)\n",
    "val_df.rename(columns = {\"input\": \"Text\"}, inplace = True)\n",
    "val_df.rename(columns = {\"summary_onesent\": \"Summary\"}, inplace = True)\n",
    "#val_df.rename(columns = {\"summary_per_20\": \"Summary\"}, inplace = True)\n",
    "print(len(train_df), len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36059af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb240ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10번째 row만 추출\n",
    "# train_df = train_df.iloc[range(0, len(train_df), 2)]\n",
    "train_df = train_df.iloc[::2, :]\n",
    "#val_df = val_df.iloc[range(0, len(val_df), 2)]\n",
    "print(len(train_df), len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e16d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 제거\n",
    "    sentence = re.sub(\"'\",'', sentence) # 따옴표 제거\n",
    "    sentence = re.sub('\\n','', sentence) # \\n \" 제거\n",
    "    sentence = re.sub('.{2,3}\\W{0,1}기자','', sentence) # 기자 이름 제거\n",
    "    sentence = re.sub(r'[?.!,][/?.!,]', '', sentence) # 여러개 문장 부호를 하나의 문장부호로 바꿉니다\n",
    "    sentence = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-z0-9]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3235b2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125690/125690 [00:26<00:00, 4766.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "from tqdm import tqdm\n",
    "clean_text = []\n",
    "\n",
    "for s in tqdm(train_df['Text']):\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "    \n",
    "train_df['Text'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e4aa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125690/125690 [00:03<00:00, 38312.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 headlines 데이터에 대한 전처리 \n",
    "clean_headlines = []\n",
    "\n",
    "for s in tqdm(train_df['Summary']):\n",
    "      clean_headlines.append(preprocess_sentence(s))\n",
    "        \n",
    "train_df['Summary'] = clean_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bad850e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18300/18300 [00:03<00:00, 4842.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "\n",
    "clean_text_val = []\n",
    "\n",
    "for s in tqdm(val_df['Text']):\n",
    "    clean_text_val.append(preprocess_sentence(s))\n",
    "    \n",
    "val_df['Text'] = clean_text_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c138a4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18300/18300 [00:00<00:00, 36627.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 headlines 데이터에 대한 전처리 \n",
    "clean_headlines_val = []\n",
    "\n",
    "for s in tqdm(val_df['Summary']):\n",
    "      clean_headlines_val.append(preprocess_sentence(s))\n",
    "        \n",
    "val_df['Summary'] = clean_headlines_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8510cd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf3d60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_index 사용\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3a036d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_df) \n",
    "#val_len = len(val_df) // 2\n",
    "val_data = Dataset.from_pandas(val_df.iloc[::2, :])\n",
    "test_data = Dataset.from_pandas(val_df.iloc[1::2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95e3d6b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 125690)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 9150)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 9150)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b86f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bertshared-kor-base\")\n",
    "#parameter setting\n",
    "encoder_max_length=512\n",
    "decoder_max_length=64\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093efc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'kykim/bertshared-kor-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d9c220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06eeaa8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 16453, 14121, 4542, 14301, 2005, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"안녕하세요 처음 뵙겠습니다 !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ab9c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 512\n",
    "max_target = 128\n",
    "batch_size = 16\n",
    "model_checkpoint = \"kykim/bertshared-kor-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de6e0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(batch[\"Text\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "    outputs = tokenizer(batch[\"Summary\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "    # because RoBERTa automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afae8bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = train_data.select(range(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef16293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_data = val_data.select(range(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "847fd6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    #get all the dialogues\n",
    "    inputs = [dialogue for dialogue in data_to_process['Text']]\n",
    "    #tokenize the dialogues\n",
    "    model_inputs = tokenizer(inputs,  max_length=encoder_max_length, padding='max_length', truncation=True)\n",
    "    #tokenize the summaries\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        targets = tokenizer(data_to_process['Summary'], max_length=decoder_max_length, padding='max_length', truncation=True)\n",
    "\n",
    "    #set labels\n",
    "    model_inputs['labels'] = targets['input_ids']\n",
    "    #return the tokenized data\n",
    "    #input_ids, attention_mask and labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fb99ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589f50bb8b9c4111a9f50cf40b3559b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7856 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9679ca4b83744bd88f12139dfc072817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/572 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#processing training data\n",
    "train_data = train_data.map(\n",
    "    preprocess_data, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"Text\", \"Summary\"])\n",
    "\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"],)\n",
    "\n",
    "#processing validation data\n",
    "val_data = val_data.map(\n",
    "    preprocess_data, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"Text\", \"Summary\"])\n",
    "\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dea19bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'input_ids': tensor([[    2, 18127, 15263,  8051, 18597, 14041, 20895,  8008, 14083,  2240,\n",
       "           8268, 15995, 23482, 14686,  5036, 16619, 25313,  8273,  5957, 18623,\n",
       "           8010, 41315, 14199, 27045, 15727, 14208, 14238, 14275, 13979, 21310,\n",
       "           8283,  8847,  8107, 15354,  8055, 14393, 15416, 25416, 14275, 35356,\n",
       "           8112, 14053, 39529, 21048, 31906, 15299, 14367, 14671, 14041, 15022,\n",
       "          25416, 14275, 35356,  8112, 39529, 21048, 29285, 24894,  8010, 41315,\n",
       "          14199, 27045, 15727, 14254, 20549, 14090, 18263, 22348,  8048, 14178,\n",
       "           2173,  8054, 28326, 18263, 15996, 14217, 25892,  8043, 18263, 37107,\n",
       "          15818, 21991, 22945,  3547, 17088, 18263, 22348, 29500, 17287, 28034,\n",
       "           8051,  4349, 14681, 33070,  2019, 14063, 18919, 28086, 37982, 16496,\n",
       "          16619, 25313, 14621, 15099, 14515, 14850, 20108, 25597,  5099, 41248,\n",
       "          31738,  8010, 41315, 14199, 27045, 15727, 14208, 18278,  8018,  5555,\n",
       "          14876, 21407,  8087,  2469,  8255, 22339, 20349, 14026, 17200, 20921,\n",
       "           8078, 14119, 25416,  8544, 15416, 39529, 21048, 15996,  8107, 26403,\n",
       "          14208, 24894,  8010, 41315, 14199, 16420, 15727, 14254, 14363,  8283,\n",
       "           8847, 23071, 15275, 18359, 14117, 35845, 18586, 15214, 18586, 35845,\n",
       "           8267,  3408, 23856,  8514, 16512,  8107, 25239, 24024,  8051, 16025,\n",
       "          13975,  2199, 14246, 37160, 16512, 27158,  8158, 24553, 15839, 14083,\n",
       "          25583, 24478, 34543,  8289,  5858,  8184, 26709,  8008, 14083, 37543,\n",
       "          18565,  3547, 15916, 19860, 14083, 14521,  8158,  8031, 14026,  4134,\n",
       "           8113, 16872,  3408, 20108, 21874, 19085, 15300,  8010, 41315, 14199,\n",
       "          27045, 15727, 14254, 17394, 16733, 14106, 18169,  8170,  8267, 21620,\n",
       "          24553, 15839, 14083, 18565,  8144, 24398, 16733,  8010, 41315, 14199,\n",
       "          27045, 15727, 15022, 23856,  8514, 16512,  8008, 15824, 24024,  8051,\n",
       "          16025, 14006, 14250, 20108,  8069,  8438, 31738,  8048, 14742, 23786,\n",
       "          14102, 26299,  8271, 22683, 27549, 40166,     3,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'labels': tensor([[    2, 20895, 21310,  8283,  8847,  8107, 15354,  8055, 40135, 25416,\n",
       "          14275, 35356,  8112, 39529, 21048, 29285, 24894, 13973, 14686,  5036,\n",
       "          16619, 25313,  8273,  5957, 18623,  8150, 14199, 27045, 14035,     3,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing training data\n",
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"Text\", \"Summary\"])\n",
    "\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],)\n",
    "\n",
    "#processing validation data\n",
    "val_data = val_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"Text\", \"Summary\"])\n",
    "\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ecbee45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d534d3251b2343e4beb9c962bed5d1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b15a30e4b05476fa72ffb4e64827fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_data = train_data.map(preprocess_data, batched = True, remove_columns=[\"Text\", \"Summary\"])\n",
    "#val_data = val_data.map(preprocess_data, batched = True, remove_columns=[\"Text\", \"Summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d16a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4c97c9fda5405baa9139c3e2c59861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/589M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    }
   ],
   "source": [
    "# 인코더 모델 불러오기\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "#bertshared =EncoderDecoderModel.from_pretrained(\"kykim/bertshared-kor-base\")\n",
    "#bertshared = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = EncoderDecoderModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c694c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73bfb5cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderConfig {\n",
       "  \"_commit_hash\": \"8ea27677d006a547aee2a7753d1ff18a0346860c\",\n",
       "  \"_name_or_path\": \"kykim/bertshared-kor-base\",\n",
       "  \"architectures\": [\n",
       "    \"EncoderDecoderModel\"\n",
       "  ],\n",
       "  \"decoder\": {\n",
       "    \"_name_or_path\": \"\",\n",
       "    \"add_cross_attention\": true,\n",
       "    \"architectures\": [\n",
       "      \"BertForMaskedLM\"\n",
       "    ],\n",
       "    \"attention_probs_dropout_prob\": 0.1,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": 2,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"classifier_dropout\": null,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": 2,\n",
       "    \"directionality\": \"bidi\",\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"early_stopping\": false,\n",
       "    \"embedding_size\": 768,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 3,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"gradient_checkpointing\": false,\n",
       "    \"hidden_act\": \"gelu\",\n",
       "    \"hidden_dropout_prob\": 0.1,\n",
       "    \"hidden_size\": 768,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 3072,\n",
       "    \"is_decoder\": true,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"layer_norm_eps\": 1e-12,\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"max_position_embeddings\": 512,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"bert\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_hidden_layers\": 12,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": 0,\n",
       "    \"pooler_fc_size\": 768,\n",
       "    \"pooler_num_attention_heads\": 12,\n",
       "    \"pooler_num_fc_layers\": 3,\n",
       "    \"pooler_size_per_head\": 128,\n",
       "    \"pooler_type\": \"first_token_transform\",\n",
       "    \"position_embedding_type\": \"absolute\",\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"sep_token_id\": 3,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"transformers_version\": \"4.24.0\",\n",
       "    \"type_vocab_size\": 2,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 42000\n",
       "  },\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder\": {\n",
       "    \"_name_or_path\": \"\",\n",
       "    \"add_cross_attention\": false,\n",
       "    \"architectures\": [\n",
       "      \"BertForMaskedLM\"\n",
       "    ],\n",
       "    \"attention_probs_dropout_prob\": 0.1,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": 2,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"classifier_dropout\": null,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"directionality\": \"bidi\",\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"early_stopping\": false,\n",
       "    \"embedding_size\": 768,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 3,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"gradient_checkpointing\": false,\n",
       "    \"hidden_act\": \"gelu\",\n",
       "    \"hidden_dropout_prob\": 0.1,\n",
       "    \"hidden_size\": 768,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 3072,\n",
       "    \"is_decoder\": false,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"layer_norm_eps\": 1e-12,\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"max_position_embeddings\": 512,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"bert\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_hidden_layers\": 12,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": 0,\n",
       "    \"pooler_fc_size\": 768,\n",
       "    \"pooler_num_attention_heads\": 12,\n",
       "    \"pooler_num_fc_layers\": 3,\n",
       "    \"pooler_size_per_head\": 128,\n",
       "    \"pooler_type\": \"first_token_transform\",\n",
       "    \"position_embedding_type\": \"absolute\",\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"sep_token_id\": 3,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"transformers_version\": \"4.24.0\",\n",
       "    \"type_vocab_size\": 2,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 42000\n",
       "  },\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"length_penalty\": 1.5,\n",
       "  \"min_length\": 30,\n",
       "  \"model_type\": \"encoder-decoder\",\n",
       "  \"no_repeat_ngram_size\": 2,\n",
       "  \"num_beams\": 4,\n",
       "  \"tie_encoder_decoder\": true,\n",
       "  \"transformers_version\": null,\n",
       "  \"vocab_size\": 42000\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2337147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens\n",
    "#from transformers import EncoderDecoderConfig\n",
    "#model.config.decoder_start_token_id = tokenizer.bos_token_id   \n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 128\n",
    "model.config.min_length = 32\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.early_stopping = True\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4\n",
    "model.config.vocab_size = model.config.encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01b9fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Seq2SeqTrainingArguments(TrainingArguments):\n",
    "    label_smoothing: Optional[float] = field(\n",
    "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (if not zero).\"}\n",
    "    )\n",
    "    sortish_sampler: bool = field(default=False, metadata={\"help\": \"Whether to SortishSamler or not.\"})\n",
    "    predict_with_generate: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
    "    )\n",
    "    adafactor: bool = field(default=False, metadata={\"help\": \"whether to use adafactor\"})\n",
    "    encoder_layerdrop: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Encoder layer dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    decoder_layerdrop: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Decoder layer dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    dropout: Optional[float] = field(default=None, metadata={\"help\": \"Dropout probability. Goes into model.config.\"})\n",
    "    attention_dropout: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Attention dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    lr_scheduler: Optional[str] = field(\n",
    "        default=\"linear\", metadata={\"help\": f\"Which lr scheduler to use.\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa31ac75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d297943878cd405e9596fb296ea70d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load rouge for validation\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a220d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8b1b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/aiffel/aiffel/aiffelthon/ch/\",\n",
    "    num_train_epochs=1,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,  # demo\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-05,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"/aiffel/aiffel/aiffelthon/logs\",\n",
    "    logging_steps=2000,\n",
    "    save_total_limit=3,\n",
    "    fp16 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d0c5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/aiffel/aiffel/aiffelthon/ch/\",\n",
    "    num_train_epochs=1, \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-05,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=2000,  # set to 2000 for full training\n",
    "    save_steps=500,  # set to 500 for full training\n",
    "    eval_steps=500,  # set to 7500 for full training\n",
    "    warmup_steps=500,  # set to 3000 for full training\n",
    "    logging_dir=\"/aiffel/aiffel/aiffelthon/logs\",\n",
    "    save_total_limit=3,\n",
    "    fp16=True,)\n",
    "    #max_steps=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cbf14af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model, \n",
    "    args = training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586adcb",
   "metadata": {},
   "source": [
    "### DataCollatorForSeq2Seq의 역할\n",
    "-  decoder 모델이 있어서 model이라는 인자를 추가하면, pretrained 모델이 `prepare_decoder_input_ids_from_labels`를 가지고 있는 경우에 `decoder_input_ids`를 만든다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f08dfde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 125690\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15712\n",
      "  Number of trainable parameters = 147298320\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1794: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15712' max='15712' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15712/15712 2:16:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.902800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.792600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.776700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.765600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-6000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-6500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-7000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-1000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-1500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-2000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-2500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-3000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-3500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-4000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-4500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1794: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-6500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-5000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1794: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-7000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-5500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-7500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-6000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-8000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-6500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-8500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-7000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-9000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-7500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-9500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-8000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1794: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-10000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-8500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-10500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-9000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-11000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-9500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-11500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-10000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-12000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-12000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-10500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-12500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-12500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-11000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-13000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-13000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-11500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-13500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-13500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-12000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-14000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-14000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-12500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-14500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-14500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-13000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-15000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-15000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-13500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-15500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-15500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-14000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1794: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15712, training_loss=0.860140915072377, metrics={'train_runtime': 8198.0356, 'train_samples_per_second': 15.332, 'train_steps_per_second': 1.917, 'total_flos': 4.426768226893824e+16, 'train_loss': 0.860140915072377, 'epoch': 1.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41e0cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--kykim--bertshared-kor-base/snapshots/8ea27677d006a547aee2a7753d1ff18a0346860c/config.json\n",
      "Model config EncoderDecoderConfig {\n",
      "  \"_commit_hash\": \"8ea27677d006a547aee2a7753d1ff18a0346860c\",\n",
      "  \"_name_or_path\": \"kykim/bertshared-kor-base\",\n",
      "  \"architectures\": [\n",
      "    \"EncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": 2,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 1.5,\n",
      "  \"min_length\": 30,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 2,\n",
      "  \"num_beams\": 4,\n",
      "  \"tie_encoder_decoder\": true,\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 42000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /aiffel/.cache/huggingface/hub/models--kykim--bertshared-kor-base/snapshots/8ea27677d006a547aee2a7753d1ff18a0346860c/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /aiffel/.cache/huggingface/hub/models--kykim--bertshared-kor-base/snapshots/8ea27677d006a547aee2a7753d1ff18a0346860c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--kykim--bertshared-kor-base/snapshots/8ea27677d006a547aee2a7753d1ff18a0346860c/config.json\n",
      "Model config EncoderDecoderConfig {\n",
      "  \"_commit_hash\": \"8ea27677d006a547aee2a7753d1ff18a0346860c\",\n",
      "  \"_name_or_path\": \"kykim/bertshared-kor-base\",\n",
      "  \"architectures\": [\n",
      "    \"EncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": 2,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 1.5,\n",
      "  \"min_length\": 30,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 2,\n",
      "  \"num_beams\": 4,\n",
      "  \"tie_encoder_decoder\": true,\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 42000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--kykim--bertshared-kor-base/snapshots/8ea27677d006a547aee2a7753d1ff18a0346860c/config.json\n",
      "Model config EncoderDecoderConfig {\n",
      "  \"_commit_hash\": \"8ea27677d006a547aee2a7753d1ff18a0346860c\",\n",
      "  \"_name_or_path\": \"kykim/bertshared-kor-base\",\n",
      "  \"architectures\": [\n",
      "    \"EncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": 2,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 1.5,\n",
      "  \"min_length\": 30,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 2,\n",
      "  \"num_beams\": 4,\n",
      "  \"tie_encoder_decoder\": true,\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 42000\n",
      "}\n",
      "\n",
      "loading configuration file /aiffel/aiffel/aiffelthon/ch/checkpoint-15500/config.json\n",
      "Model config EncoderDecoderConfig {\n",
      "  \"_commit_hash\": null,\n",
      "  \"_name_or_path\": \"/aiffel/aiffel/aiffelthon/ch/checkpoint-15500/\",\n",
      "  \"architectures\": [\n",
      "    \"EncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": 2,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"eos_token_id\": 3,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 32,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"tie_encoder_decoder\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 42000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file /aiffel/aiffel/aiffelthon/ch/checkpoint-15500/pytorch_model.bin\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
      "\n",
      "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /aiffel/aiffel/aiffelthon/ch/checkpoint-15500/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 128 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d075c5576a4d4c86379d1928044750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/572 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('/aiffel/aiffel/aiffelthon/ch/checkpoint-15500/').to(\"cuda\")\n",
    "batch_size = 16\n",
    "\n",
    "# map data correctly\n",
    "def generate_summary(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    inputs = tokenizer(batch[\"Text\"], padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"Text\"])\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"Summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ac0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(pred_str), type(label_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e88bddba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted sentence :  소비자의 외식 트렌드는 시시각각 변하고 변화에 맞게 신메뉴가 개발되고 신사업이 떴다가 사라지는 현상은 반복되며 소비자의 외식 소비형태 데이터 못지않게 향후 향후 향후 앞으로의 향후는 반복된다\n",
      "real sentence :  농림식품부 주관으로 열린 2020년 떠오르는 외식 경향 발표의 핵심 키워드는 그린 오션 바이 미 포 미 멀티 스트리밍 편리미엄 외식의 확대이다\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for num in range(5,4):\n",
    "    print('predicted sentence : ',pred_str[num])\n",
    "    print('real sentence : ', label_str[num])\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aac5c236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted sentence :  손 식품의약품안전처장은 어패류 관련해서 우려들이 많은데 콜콜류 관련 우려들이 많은데 콜 콜 콜에 대한 우려들이 많다고 했다\n",
      "\n",
      "\n",
      "real sentence :  손 식품의약품안전처장은 식중독이 발생하지 않게 어패류를 깨끗한 수돗물로 잘 세척해 오염된 부분을 제거하고 교차오염이 되지 않도록 손 등을 잘 씻는 행동요령을 잘 지켜달라고 했다\n",
      "\n",
      "\n",
      "Text {'Text': '위원장 양승조 처장님도 한 말씀 하시지요 식품의약품안전처장 손문기 어패류 관련해서 좀 우려들이 많으신데 일단 콜레라라든지 비브리오 오염이 되더라도 깨끗한 수돗물로 잘 세척해서 그 부분을 제거하고 그다음에 손과 생선 칼 도마들이 교차오염 되지 않도록 잘 씻어서 하시면 식중독이나 이런 피해가 발생하지 않을 걸로 보고 있습니다 그래서 그런 행동요령을 잘 지켜 주시기 바라고 그래도 좀 의심되는 경우에는 잘 끓이고 익혀서 드시면 크게 문제는 없을 것으로 보고 있습니다 저희도 여름철 안전관리를 위해서 최선을 다하도록 하겠습니다 위원장 양승조 질병관리본부장님도 한 말씀 하세요 질병관리본부장 정기석 저희는 한 박자 빠른 선제 대응 그다음에 과하다 싶을 정도의 대응을 하는 것을 원칙으로 삼고 일을 열심히 하고 있습니다 그래서 앞으로 조금 더 신뢰받는 기관이 될 수 있도록 그렇게 해서 여러분들께서 그렇게 걱정을 많이 하시는 일이 없도록 더 철두철미하게 일하도록 하겠습니다 감사합니다 위원장 양승조 수고하셨습니다 서면질의하신 위원님이 계십니다 강석진 위원님 기동민 위원님 김명연 위원님 송석준 위원님 인재근 위원님 남인순 위원님 양승조 위원님 박인숙 위원님 정춘숙 위원님으로부터 서면질의서가 제출되었습니다 소상하고 성실한 답변서를 작성하여 9월 5일까지 제출해 주시기 바랍니다 서면질의와 답변 내용은 회의록에 게재하도록 하겠습니다 여러 위원님들 수고 많이 하셨습니다 특히 끝까지 남아 주신 인재근 위원님 김광수 위원님 최도자 위원님 김승희 위원님 윤소하 위원님 정말 수고 많으셨다는 존경과 감사의 인사를 드립니다 장관님 처장님을 비롯한 관계 직원분들도 수고 많으셨습니다 오늘 회의는 이것으로 모두 마치겠습니다 산회를 선포합니다', 'Summary': '손 식품의약품안전처장은 식중독이 발생하지 않게 어패류를 깨끗한 수돗물로 잘 세척해 오염된 부분을 제거하고 교차오염이 되지 않도록 손 등을 잘 씻는 행동요령을 잘 지켜달라고 했다'}\n"
     ]
    }
   ],
   "source": [
    "num = 2500\n",
    "print('predicted sentence : ',pred_str[num])\n",
    "print('\\n')\n",
    "print('real sentence : ', label_str[num])\n",
    "print('\\n')\n",
    "print('Text', test_data[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b30b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(0, len(label_str), 100):\n",
    "    print('predicted sentence : ',pred_str[num])\n",
    "    print('real sentence : ', label_str[num])\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "208ef4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE 1 SCORE:  Score(precision=0.08858293650615254, recall=0.10559894613583143, fmeasure=0.08848478563218844)\n",
      "ROUGE 2 SCORE:  Score(precision=0.016476199405080792, recall=0.02023315118397086, fmeasure=0.016522825463291396)\n",
      "ROUGE L SCORE:  Score(precision=0.08795556315180239, recall=0.10482220921155352, fmeasure=0.08784555763278679)\n"
     ]
    }
   ],
   "source": [
    "print(\"ROUGE 1 SCORE: \",rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\"])[\"rouge1\"].mid)\n",
    "print(\"ROUGE 2 SCORE: \",rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n",
    "print(\"ROUGE L SCORE: \",rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rougeL\"])[\"rougeL\"].mid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
