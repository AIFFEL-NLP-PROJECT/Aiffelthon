{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cf3f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97457480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('~/aiffel/aiffelthon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6887474",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.21.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: datasets==1.0.2 in /opt/conda/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.21.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.26.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (0.3.4)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.0.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (4.62.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "!pip install datasets==1.0.2\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8360cbdf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers==4.24.0 in /opt/conda/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (1.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2021.11.10)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2021.10.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fb93863",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformer-utils in /opt/conda/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.62.3)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (0.11.2)\n",
      "Requirement already satisfied: colorcet in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (3.0.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (1.9.1+cu111)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.24.0)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /opt/conda/lib/python3.9/site-packages (from colorcet->transformer-utils) (0.4.8)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.21.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (3.4.3)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->transformer-utils) (4.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (3.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (8.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->seaborn->transformer-utils) (2021.3)\n",
      "Requirement already satisfied: param>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from pyct>=0.4.4->colorcet->transformer-utils) (1.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.0.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn->transformer-utils) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformer-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3886616",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3016b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "#Tokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "#Encoder-Decoder Model\n",
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "#Training\n",
    "from seq2seq_trainer import Seq2SeqTrainer\n",
    "from transformers import TrainingArguments\n",
    "#from seq2seq_training_args import Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 지정\n",
    "#%cd ~/aiffel/aiffelthon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6218f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip ~/aiffel/aiffelthon/csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752942d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/legacy/seq2seq/seq2seq_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ab677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/huggingface/transformers/blob/main/src/transformers/utils/import_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c004ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75278965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/configuration_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3c33837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125690 18300\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "train_df = pd.read_csv('/aiffel/aiffel/aiffelthon/csv/train_Sum1.csv')\n",
    "#train_df.drop(labels = 'Unnamed: 0', axis = 1, inplace = True)\n",
    "train_df.rename(columns = {\"input\": \"Text\"}, inplace = True)\n",
    "train_df.rename(columns = {\"sentence_onesent\": \"Summary\"}, inplace = True)\n",
    "#train_df.rename(columns = {\"sentence_per_20\": \"Summary\"}, inplace = True)\n",
    "\n",
    "val_df = pd.read_csv('/aiffel/aiffel/aiffelthon/csv/val_Sum1.csv')\n",
    "#val_df.drop(labels = 'Unnamed: 0', axis = 1, inplace = True)\n",
    "val_df.rename(columns = {\"input\": \"Text\"}, inplace = True)\n",
    "val_df.rename(columns = {\"summary_onesent\": \"Summary\"}, inplace = True)\n",
    "#val_df.rename(columns = {\"summary_per_20\": \"Summary\"}, inplace = True)\n",
    "print(len(train_df), len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd07cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "476e2026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62845 9150\n"
     ]
    }
   ],
   "source": [
    "# 10번째 row만 추출\n",
    "train_df = train_df.iloc[range(0, len(train_df), 2)]\n",
    "val_df = val_df.iloc[range(0, len(val_df), 2)]\n",
    "print(len(train_df), len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ec85289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 제거\n",
    "    sentence = re.sub(\"'\",'', sentence) # 따옴표 제거\n",
    "    sentence = re.sub('\\n','', sentence) # \\n \" 제거\n",
    "    sentence = re.sub('.{2,3}\\W{0,1}기자','', sentence) # 기자 이름 제거\n",
    "    sentence = re.sub(r'[?.!,][/?.!,]', '', sentence) # 여러개 문장 부호를 하나의 문장부호로 바꿉니다\n",
    "    sentence = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-z0-9]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61d24641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62845/62845 [00:12<00:00, 4843.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "from tqdm import tqdm\n",
    "clean_text = []\n",
    "\n",
    "for s in tqdm(train_df['Text']):\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "    \n",
    "train_df['Text'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "faf0bdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62845/62845 [00:01<00:00, 39742.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 headlines 데이터에 대한 전처리 \n",
    "clean_headlines = []\n",
    "\n",
    "for s in tqdm(train_df['Summary']):\n",
    "      clean_headlines.append(preprocess_sentence(s))\n",
    "        \n",
    "train_df['Summary'] = clean_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3176b71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9150/9150 [00:01<00:00, 5006.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "\n",
    "clean_text_val = []\n",
    "\n",
    "for s in tqdm(val_df['Text']):\n",
    "    clean_text_val.append(preprocess_sentence(s))\n",
    "    \n",
    "val_df['Text'] = clean_text_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f56f05f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9150/9150 [00:00<00:00, 42876.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 headlines 데이터에 대한 전처리 \n",
    "clean_headlines_val = []\n",
    "\n",
    "for s in tqdm(val_df['Summary']):\n",
    "      clean_headlines_val.append(preprocess_sentence(s))\n",
    "        \n",
    "val_df['Summary'] = clean_headlines_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b03e46c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>내일 주말을 앞두고 있습니다 코로나19에 대한 경각심을 유지하고 생활 속 거리두기를...</td>\n",
       "      <td>코로나19 집단감염의 위험요인인 밀폐되고 밀집한 다중이용시설을 자제하고 생활 속 거...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아울러 생활방역 시행일로부터 코로나19 감염위기 상황 종료 시까지 생활방역대책본부를...</td>\n",
       "      <td>생활방역대책본부를 구성하여 추진 상황과 주요 대책을 논의할 계획이고 지자체와 방역당...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>특별히 교육시설 종교시설 실내 체육시설 그리고 의료기관과 같이 많은 사람들이 이용하...</td>\n",
       "      <td>사람들이 많이 이용하는 시설 종사자는 반드시 진단 검사를 받아야 하며 검사 결과가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그러니까 단체방 중에서도 회원가입이라든지 누군가의 허락을 받아서 들어가는 단체방 같...</td>\n",
       "      <td>법 시행령이 통과되면 6개월 이후에 시행되지만 기술적 관리 조치에는 준비가 필요하므...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>무증상 환자들도 감염력을 가지고 있어서 지역사회의 다양한 공간에서 2차 또는 3차까...</td>\n",
       "      <td>치명률의 세대 간 편차 등의 특성 때문에 전문가들은 백신이나 치료제 개발 전까지 코...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  내일 주말을 앞두고 있습니다 코로나19에 대한 경각심을 유지하고 생활 속 거리두기를...   \n",
       "2  아울러 생활방역 시행일로부터 코로나19 감염위기 상황 종료 시까지 생활방역대책본부를...   \n",
       "4  특별히 교육시설 종교시설 실내 체육시설 그리고 의료기관과 같이 많은 사람들이 이용하...   \n",
       "6  그러니까 단체방 중에서도 회원가입이라든지 누군가의 허락을 받아서 들어가는 단체방 같...   \n",
       "8  무증상 환자들도 감염력을 가지고 있어서 지역사회의 다양한 공간에서 2차 또는 3차까...   \n",
       "\n",
       "                                             Summary  \n",
       "0  코로나19 집단감염의 위험요인인 밀폐되고 밀집한 다중이용시설을 자제하고 생활 속 거...  \n",
       "2  생활방역대책본부를 구성하여 추진 상황과 주요 대책을 논의할 계획이고 지자체와 방역당...  \n",
       "4  사람들이 많이 이용하는 시설 종사자는 반드시 진단 검사를 받아야 하며 검사 결과가 ...  \n",
       "6  법 시행령이 통과되면 6개월 이후에 시행되지만 기술적 관리 조치에는 준비가 필요하므...  \n",
       "8  치명률의 세대 간 편차 등의 특성 때문에 전문가들은 백신이나 치료제 개발 전까지 코...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67766c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>새해가 밝았다 또 다른 한 해가 시작되었다 눈이 내린 하얀 설원이 앞에 펼쳐져 있는...</td>\n",
       "      <td>새해라는 미지의 시간에 남길 나의 발자국은 다른 이들에게는 이정표가 될 것이다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김정은 국무위원장이 지난달 28일부터 31일까지 열린 노동당 중앙위원회 전원회의 보...</td>\n",
       "      <td>노동당 중앙위원회 전원회의 보고에서 북한의 김 국무위원장이 새로운 전략무기를 목격하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>스웨덴에서는 지갑을 몸에 지니지 않아도 일상생활에 불편을 겪지 않는 이들이 있다 엄...</td>\n",
       "      <td>스웨덴에서는 지갑을 몸에 지니지 않아도 엄지와 검지 사이의 손등 표면에 마이크로칩을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>핵은 손에 든 채 제재는 해제해 달라는 북한식 셈법은 핵확산금지조약에 정면 배치된다...</td>\n",
       "      <td>북 미 협상에 맥이 빠지자 중국과 러시아는 어느 정도 동력을 유지할 수 있는 6자 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>안철수 전 대표가 그분들을 만날 계획은 현재로썬 전혀 없습니다 2일 안철수 전 국민...</td>\n",
       "      <td>이 인사는 안 전 대표가 황 대표와 유 전 대표와는 만날 계획은 없으며 설 명절 전...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  새해가 밝았다 또 다른 한 해가 시작되었다 눈이 내린 하얀 설원이 앞에 펼쳐져 있는...   \n",
       "2  김정은 국무위원장이 지난달 28일부터 31일까지 열린 노동당 중앙위원회 전원회의 보...   \n",
       "4  스웨덴에서는 지갑을 몸에 지니지 않아도 일상생활에 불편을 겪지 않는 이들이 있다 엄...   \n",
       "6  핵은 손에 든 채 제재는 해제해 달라는 북한식 셈법은 핵확산금지조약에 정면 배치된다...   \n",
       "8  안철수 전 대표가 그분들을 만날 계획은 현재로썬 전혀 없습니다 2일 안철수 전 국민...   \n",
       "\n",
       "                                             Summary  \n",
       "0        새해라는 미지의 시간에 남길 나의 발자국은 다른 이들에게는 이정표가 될 것이다  \n",
       "2  노동당 중앙위원회 전원회의 보고에서 북한의 김 국무위원장이 새로운 전략무기를 목격하...  \n",
       "4  스웨덴에서는 지갑을 몸에 지니지 않아도 엄지와 검지 사이의 손등 표면에 마이크로칩을...  \n",
       "6  북 미 협상에 맥이 빠지자 중국과 러시아는 어느 정도 동력을 유지할 수 있는 6자 ...  \n",
       "8  이 인사는 안 전 대표가 황 대표와 유 전 대표와는 만날 계획은 없으며 설 명절 전...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9086c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_index 사용\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5563b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bertshared-kor-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f98706c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>새해가 밝았다 또 다른 한 해가 시작되었다 눈이 내린 하얀 설원이 앞에 펼쳐져 있는...</td>\n",
       "      <td>새해라는 미지의 시간에 남길 나의 발자국은 다른 이들에게는 이정표가 될 것이다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>스웨덴에서는 지갑을 몸에 지니지 않아도 일상생활에 불편을 겪지 않는 이들이 있다 엄...</td>\n",
       "      <td>스웨덴에서는 지갑을 몸에 지니지 않아도 엄지와 검지 사이의 손등 표면에 마이크로칩을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>안철수 전 대표가 그분들을 만날 계획은 현재로썬 전혀 없습니다 2일 안철수 전 국민...</td>\n",
       "      <td>이 인사는 안 전 대표가 황 대표와 유 전 대표와는 만날 계획은 없으며 설 명절 전...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>일본에서 가택연금을 피해 레바논으로 몰래 도주한 카를로스 곤 전 르노 닛산 회장을 ...</td>\n",
       "      <td>전 르노 닛산 회장이 일본 가택연금을 피해 레바논으로 도주했지만 일본과 범죄인 인도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>지구를 떠난 소년은 아빠를 찾기 위해 외계 행성을 돌아다닌다 때로는 로봇을 타고 외...</td>\n",
       "      <td>1989년만 하더라도 다들 만화영화 2020년 우주의 원더키디처럼 2020년엔 우주...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9140</th>\n",
       "      <td>글쎄요 저는 문재인 대통령의 국정 수행에 대한 지지도라는 것을 과거에도 그랬지만 엄...</td>\n",
       "      <td>논리적으로 따져보면 말이 안 되기 때문에 심하게 말하면 맹목적인 지지나 무조건 지지다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9142</th>\n",
       "      <td>네 네 디젤이에요 보면 이거 미세먼지 많다고 우리 엄마들 되게 이렇게 하는 부분인데...</td>\n",
       "      <td>타다 드라이버 같은 경우 내일이라도 배차가 없을 경우 바로 해고될 수 있는 비정규직...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9144</th>\n",
       "      <td>기본적으로 지금 정부의 어떤 대응 tf는 국무총리실에서 맡고 있습니다 거기에서 외교...</td>\n",
       "      <td>일본 정부가 약속한 부분이 있지만 기본적으로 정부의 대응 tf는 국무총리실에서 맡고 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9146</th>\n",
       "      <td>한편 강화고 최후의 1인은 돌직구 발언으로 에이핑크의 마음을 울렸습니다 이에 에이핑...</td>\n",
       "      <td>성우가 꿈인 형윤이를 응원하기 위해 영국의 인기 드라마 셜록의 셜록 역을 맡은 장 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9148</th>\n",
       "      <td>지난 04월 02일 전북 익산에서 차도에 한 남자가 쓰려져 있다는 신고를 받고 11...</td>\n",
       "      <td>한국인이 가장 존경하는 직업 1위인 119 소방 공무원들의 안전은 오늘도 위협당하고 있다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4575 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0     새해가 밝았다 또 다른 한 해가 시작되었다 눈이 내린 하얀 설원이 앞에 펼쳐져 있는...   \n",
       "2     스웨덴에서는 지갑을 몸에 지니지 않아도 일상생활에 불편을 겪지 않는 이들이 있다 엄...   \n",
       "4     안철수 전 대표가 그분들을 만날 계획은 현재로썬 전혀 없습니다 2일 안철수 전 국민...   \n",
       "6     일본에서 가택연금을 피해 레바논으로 몰래 도주한 카를로스 곤 전 르노 닛산 회장을 ...   \n",
       "8     지구를 떠난 소년은 아빠를 찾기 위해 외계 행성을 돌아다닌다 때로는 로봇을 타고 외...   \n",
       "...                                                 ...   \n",
       "9140  글쎄요 저는 문재인 대통령의 국정 수행에 대한 지지도라는 것을 과거에도 그랬지만 엄...   \n",
       "9142  네 네 디젤이에요 보면 이거 미세먼지 많다고 우리 엄마들 되게 이렇게 하는 부분인데...   \n",
       "9144  기본적으로 지금 정부의 어떤 대응 tf는 국무총리실에서 맡고 있습니다 거기에서 외교...   \n",
       "9146  한편 강화고 최후의 1인은 돌직구 발언으로 에이핑크의 마음을 울렸습니다 이에 에이핑...   \n",
       "9148  지난 04월 02일 전북 익산에서 차도에 한 남자가 쓰려져 있다는 신고를 받고 11...   \n",
       "\n",
       "                                                Summary  \n",
       "0           새해라는 미지의 시간에 남길 나의 발자국은 다른 이들에게는 이정표가 될 것이다  \n",
       "2     스웨덴에서는 지갑을 몸에 지니지 않아도 엄지와 검지 사이의 손등 표면에 마이크로칩을...  \n",
       "4     이 인사는 안 전 대표가 황 대표와 유 전 대표와는 만날 계획은 없으며 설 명절 전...  \n",
       "6     전 르노 닛산 회장이 일본 가택연금을 피해 레바논으로 도주했지만 일본과 범죄인 인도...  \n",
       "8     1989년만 하더라도 다들 만화영화 2020년 우주의 원더키디처럼 2020년엔 우주...  \n",
       "...                                                 ...  \n",
       "9140    논리적으로 따져보면 말이 안 되기 때문에 심하게 말하면 맹목적인 지지나 무조건 지지다  \n",
       "9142  타다 드라이버 같은 경우 내일이라도 배차가 없을 경우 바로 해고될 수 있는 비정규직...  \n",
       "9144  일본 정부가 약속한 부분이 있지만 기본적으로 정부의 대응 tf는 국무총리실에서 맡고 있다  \n",
       "9146  성우가 꿈인 형윤이를 응원하기 위해 영국의 인기 드라마 셜록의 셜록 역을 맡은 장 ...  \n",
       "9148  한국인이 가장 존경하는 직업 1위인 119 소방 공무원들의 안전은 오늘도 위협당하고 있다  \n",
       "\n",
       "[4575 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d58d078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_df) \n",
    "#val_len = len(val_df) // 2\n",
    "val_data = Dataset.from_pandas(val_df.iloc[::2, :])\n",
    "test_data=Dataset.from_pandas(val_df.iloc[1::2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "01c7fc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 62845)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 4575)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 4575)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9bc2c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bertshared-kor-base\")\n",
    "#parameter setting\n",
    "batch_size=16  #\n",
    "encoder_max_length=256\n",
    "decoder_max_length=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65c0a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):                                                               \n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]                                               \n",
    "    inputs = tokenizer(batch[\"Text\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "    outputs = tokenizer(batch[\"Summary\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "                                                                                                        \n",
    "    batch[\"input_ids\"] = inputs.input_ids                                                               \n",
    "    batch[\"attention_mask\"] = inputs.attention_mask                                                     \n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids                                                      \n",
    "    batch[\"labels\"] = outputs.input_ids.copy()                                                          \n",
    "    # mask loss for padding                                                                             \n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]                     \n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask                                                                              \n",
    "                                                                                                         \n",
    "    return batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c91c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(batch[\"Text\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "    outputs = tokenizer(batch[\"Summary\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "    # because RoBERTa automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "700b1f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030db85056e24e1fbf4ddab30124beed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3928 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20116b930ab241d8917f269ef7c98a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#processing training data\n",
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"Text\", \"Summary\"])\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],)\n",
    "\n",
    "#processing validation data\n",
    "val_data = val_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"Text\", \"Summary\"])\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd473ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    }
   ],
   "source": [
    "# 인코더 모델 불러오기\n",
    "\n",
    "bertshared =EncoderDecoderModel.from_pretrained(\"kykim/bertshared-kor-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea7c4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens\n",
    "#from transformers import EncoderDecoderConfig\n",
    "bertshared.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "bertshared.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "bertshared.config.max_length = 32\n",
    "bertshared.config.early_stopping = True\n",
    "bertshared.config.no_repeat_ngram_size = 2\n",
    "bertshared.config.length_penalty = 2.0\n",
    "bertshared.config.num_beams = 2\n",
    "bertshared.config.vocab_size = bertshared.config.encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aefdae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Seq2SeqTrainingArguments(TrainingArguments):\n",
    "    label_smoothing: Optional[float] = field(\n",
    "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (if not zero).\"}\n",
    "    )\n",
    "    sortish_sampler: bool = field(default=False, metadata={\"help\": \"Whether to SortishSamler or not.\"})\n",
    "    predict_with_generate: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
    "    )\n",
    "    adafactor: bool = field(default=False, metadata={\"help\": \"whether to use adafactor\"})\n",
    "    encoder_layerdrop: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Encoder layer dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    decoder_layerdrop: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Decoder layer dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    dropout: Optional[float] = field(default=None, metadata={\"help\": \"Dropout probability. Goes into model.config.\"})\n",
    "    attention_dropout: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Attention dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    lr_scheduler: Optional[str] = field(\n",
    "        default=\"linear\", metadata={\"help\": f\"Which lr scheduler to use.\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a95358ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rouge for validation\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f71b9d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 62845\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11784\n",
      "  Number of trainable parameters = 147298320\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11784' max='11784' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11784/11784 1:27:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-2500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-3000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-3500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-1000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-1500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1794: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-2000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1794: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-2500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-3000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-3500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-4000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-4500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-6500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-5000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-7000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-5500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-7500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-6000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1794: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-8000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-6500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-8500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-7000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-9000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-7500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-9500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-8000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-10000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-8500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-10500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-9000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-11000\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-9500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffelthon/ch/checkpoint-11500\n",
      "Configuration saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffelthon/ch/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/aiffelthon/ch/checkpoint-10000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11784, training_loss=0.00025422160734358645, metrics={'train_runtime': 5243.5999, 'train_samples_per_second': 35.955, 'train_steps_per_second': 2.247, 'total_flos': 3.320076170170368e+16, 'train_loss': 0.00025422160734358645, 'epoch': 3.0})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/aiffel/aiffel/aiffelthon/ch/\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=2000,  # set to 2000 for full training\n",
    "    save_steps=500,  # set to 500 for full training\n",
    "    eval_steps=7500,  # set to 7500 for full training\n",
    "    warmup_steps=3000,  # set to 3000 for full training\n",
    "    #max_steps=16, # delete for full training,\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,)\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=bertshared,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "011ae8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /aiffel/.cache/huggingface/hub/models--kykim--bertshared-kor-base/snapshots/8ea27677d006a547aee2a7753d1ff18a0346860c/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /aiffel/.cache/huggingface/hub/models--kykim--bertshared-kor-base/snapshots/8ea27677d006a547aee2a7753d1ff18a0346860c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--kykim--bertshared-kor-base/snapshots/8ea27677d006a547aee2a7753d1ff18a0346860c/config.json\n",
      "Model config EncoderDecoderConfig {\n",
      "  \"_commit_hash\": \"8ea27677d006a547aee2a7753d1ff18a0346860c\",\n",
      "  \"_name_or_path\": \"kykim/bertshared-kor-base\",\n",
      "  \"architectures\": [\n",
      "    \"EncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": 2,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 1.5,\n",
      "  \"min_length\": 30,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 2,\n",
      "  \"num_beams\": 4,\n",
      "  \"tie_encoder_decoder\": true,\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 42000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--kykim--bertshared-kor-base/snapshots/8ea27677d006a547aee2a7753d1ff18a0346860c/config.json\n",
      "Model config EncoderDecoderConfig {\n",
      "  \"_commit_hash\": \"8ea27677d006a547aee2a7753d1ff18a0346860c\",\n",
      "  \"_name_or_path\": \"kykim/bertshared-kor-base\",\n",
      "  \"architectures\": [\n",
      "    \"EncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": 2,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 1.5,\n",
      "  \"min_length\": 30,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 2,\n",
      "  \"num_beams\": 4,\n",
      "  \"tie_encoder_decoder\": true,\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 42000\n",
      "}\n",
      "\n",
      "loading configuration file /aiffel/aiffel/aiffelthon/ch/checkpoint-11500/config.json\n",
      "Model config EncoderDecoderConfig {\n",
      "  \"_commit_hash\": null,\n",
      "  \"_name_or_path\": \"kykim/bertshared-kor-base\",\n",
      "  \"architectures\": [\n",
      "    \"EncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": 2,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 2,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embedding_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 3,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": 3,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.24.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 42000\n",
      "  },\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 32,\n",
      "  \"min_length\": 30,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 2,\n",
      "  \"num_beams\": 2,\n",
      "  \"tie_encoder_decoder\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 42000\n",
      "}\n",
      "\n",
      "loading weights file /aiffel/aiffel/aiffelthon/ch/checkpoint-11500/pytorch_model.bin\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
      "\n",
      "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /aiffel/aiffel/aiffelthon/ch/checkpoint-11500/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 32 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b65807712bf4ff1a3c631e04a5ddfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bertshared-kor-base\")\n",
    "model = EncoderDecoderModel.from_pretrained('/aiffel/aiffel/aiffelthon/ch/checkpoint-11500/').to(\"cuda\")\n",
    "batch_size = 16\n",
    "\n",
    "# map data correctly\n",
    "def generate_summary(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    inputs = tokenizer(batch[\"Text\"], padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"Text\"])\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"Summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cffdb0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pred_str), type(label_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16c0ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted sentence :  일본 사법당국의 출국금지 명령이 내려져있던 카를로스 곤 전 르노닛산 회장이 레바논으로 비밀리에 도주했다.\n",
      "real sentence :  출국금지 명령을 받은 곤 전 르노닛산 회장이 레바논으로 도주하며 일본의 정치적 박해에서 빠져나왔다고 주장했는데 이에 변호인과 수사 및 출입국 당국 모두 당혹스럽다고 했다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted sentence :  통영의 하루락국인 우짜는 우동과 짜장을 한 데 한 번에 끝내 끝내는 얼큰한 음식이며 하루락의 음식이라 함은 여수의\n",
      "real sentence :  통영에서 우연히 개발한 우동과 짜장을 한 데 섞은 음식 우짜는 입맛 당기는 마력이 있는 해장 음식으로 입소문을 타면서 통영 명물로 자리 잡았다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted sentence :  문재인 대통령은 권력기관 개혁과 공정사회 개혁이 그 시작이라고 밝히며 권력기관의 개혁 및 공정 사회 개혁의 시작 이라며 강력한 변화 를 만들어내\n",
      "real sentence :  어떠한 권력기관도 국민 위에 존재할 수 없다는 문 대통령의 발언은 검찰개혁 속도전의 연장선으로 해석된다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted sentence :  한국인의 단백질 보충과 한국인 특유의 한국인 단백질 도저히 조화는 잘 어울리지 않을 것 같은 김치와 김밥, 진차한 김치찌개, 김밥에 라면에 넣어\n",
      "real sentence :  김 명예회장이 1980년대 초 우리 식문화에 어울릴 수 있도록 기름기가 들어간 살코기 참치캔 개발에 착수했고 1982년 12월에 국내 첫 참치 캔이 탄생됐다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted sentence :  세계 경제석학 진단 대니 로드릭 하버드대 케네디스쿨 국제정치경제경제학과는 부자 나라의 가난한 사람과 가난한 나라의 부자 중 어느\n",
      "real sentence :  부자 나라의 가난한 사람과 가난한 나라의 부자 중 어느 쪽이 되고 싶냐는 질문에 가난한 나라의 부자라면 호화로운 거물을 상상하기 때문에 학생들은 대부분 후자를 고른다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted sentence :  카를로스 곤 전 닛산자동차 회장이 도주한 지 나흘이 지났지만 출국 경위에 대한 미스터리가 여전히 풀리지 않고 있으며, 현재 카를\n",
      "real sentence :  전 닛산자동차 곤 회장의 출국 경위에 대해 수사를 진행 중인 일본 경찰은 곤이 누군가와 공항으로 갔을 가능성이 있다고 했는데 이에 곤은 혼자 준비했다고 밝혔다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted sentence :  퇴행성 관절염 말기로 무릎 연골이 모두 닳으면 통증이 심해져 활기찬 노년은 건강한 무릎에서 시작된다. 맞춤형 무릎 인공관절 수술\n",
      "real sentence :  퇴행성 관절염으로 무릎 연골이 모두 닳으면 인공관절을 끼워 넣어야 하는데 연세사랑병원의 3d프린터 맞춤형 인공관절 수술은 정확성을 높인 수술이다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted sentence :  추미애 신임 법무부 장관이 윤석열 검찰총장과 상견례 직후 검찰 고위 간부 인사 인사를 단행할 것으로 보이며 추 미애 첫 법무\n",
      "real sentence :  후보자 때부터 이미 검찰 인사를 정해놨다고 알려진 추 장관이 윤 총장과의 상견례를 진행하는 것은 공정한 인사 절차를 밟고 있다는 명분을 쌓기 위해서라는 해석이 나온다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted sentence :  도널드 트럼프 미국 대통령이 이란군 거물 암살로 빚어진 중동의 전운이 예사롭지 않다. 지난 3일 이란 군부 실세 거세\n",
      "real sentence :  이란 군부의 실세인 솔레이마니가 암살당하자 이란의 최고지도자와 대통령이 복수를 다짐했고 트럼프 대통령은 미국을 공격할 경우 이란 내 주요 목표물을 공격하겠다고 대응했다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted sentence :  7일로 결정된 양극을 향해 달리는 두 거대 정당과 늘어난 부동층 효과 인 보수통합 전망 7일로는 뚜렷한 선거제 앞도 알 수\n",
      "real sentence :  대선후보 지지율 1 2위인 이 총리와 황 대표는 종로 출마 의사를 밝혔고 서울 종로는 21대 총선에서 정치 1번지로 부각되고 있다\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for num in range(10):\n",
    "    print('predicted sentence : ',pred_str[num])\n",
    "    print('real sentence : ', label_str[num])\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "084d177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE 1 SCORE:  Score(precision=0.0026099737215403767, recall=0.017843351548269583, fmeasure=0.00394730023847285)\n",
      "ROUGE 2 SCORE:  Score(precision=0.0001961305440057867, recall=0.0017122040072859745, fmeasure=0.0003364788660428919)\n",
      "ROUGE L SCORE:  Score(precision=0.002561265101635002, recall=0.017715846994535517, fmeasure=0.003887713962727652)\n"
     ]
    }
   ],
   "source": [
    "print(\"ROUGE 1 SCORE: \",rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\"])[\"rouge1\"].mid)\n",
    "print(\"ROUGE 2 SCORE: \",rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n",
    "print(\"ROUGE L SCORE: \",rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rougeL\"])[\"rougeL\"].mid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
