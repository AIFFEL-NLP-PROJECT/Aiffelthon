{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91fa0dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/aiffelthon/prompt/OpenPrompt\n"
     ]
    }
   ],
   "source": [
    "%cd /aiffel/aiffel/aiffelthon/prompt/OpenPrompt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7247d162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/aiffelthon/prompt_sum/Prompt-Summarization\n"
     ]
    }
   ],
   "source": [
    "#%cd /aiffel/aiffel/aiffelthon/prompt_sum/Prompt-Summarization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72a23e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/aiffelthon/prompt/OpenPrompt/openprompt\n"
     ]
    }
   ],
   "source": [
    "%cd /aiffel/aiffel/aiffelthon/prompt/OpenPrompt/openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6beb65",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.10.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (4.20.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.96 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.1.96)\n",
      "Requirement already satisfied: tqdm>=4.62.2 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (4.62.3)\n",
      "Requirement already satisfied: tensorboardX in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (2.5.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (3.6.5)\n",
      "Requirement already satisfied: yacs in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.1.8)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.3.4)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.14.0)\n",
      "Requirement already satisfied: rouge==1.0.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (6.0.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (1.7.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge==1.0.0->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.0->-r requirements.txt (line 1)) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.0->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.0->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.0->-r requirements.txt (line 1)) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.0->-r requirements.txt (line 1)) (1.21.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.0->-r requirements.txt (line 1)) (0.11.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.0->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.10.0->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /opt/conda/lib/python3.9/site-packages (from tensorboardX->-r requirements.txt (line 5)) (3.19.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 6)) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 6)) (1.1.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
      "     |████████████████████████████████| 451 kB 5.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (0.70.12.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (1.3.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (2021.11.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (2.0.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 9)) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.10.0->-r requirements.txt (line 1)) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers>=4.10.0->-r requirements.txt (line 1)) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.10.0->-r requirements.txt (line 1)) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.10.0->-r requirements.txt (line 1)) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.10.0->-r requirements.txt (line 1)) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.10.0->-r requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (5.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (4.0.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 9)) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 9)) (2.8.2)\n",
      "Installing collected packages: responses, datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.14.0\n",
      "    Uninstalling datasets-1.14.0:\n",
      "      Successfully uninstalled datasets-1.14.0\n",
      "Successfully installed datasets-2.7.1 responses-0.18.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad2a29",
   "metadata": {},
   "source": [
    "# 1단계 작업 정의\n",
    "\n",
    "- 첫 번째 단계는 현재 NLP 작업을 결정하고 데이터가 어떻게 생겼는지, 데이터에서 원하는 것이 무엇인지 생각하는 것입니다! 즉, 이 단계의 본질은 작업의 classses와 를 결정하는 InputExample것입니다. 단순화를 위해 감정 분석을 예로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63186b1d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.20.1\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "     |████████████████████████████████| 4.4 MB 6.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.20.1) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "     |████████████████████████████████| 6.6 MB 65.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.20.1) (3.4.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
      "     |████████████████████████████████| 182 kB 81.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.20.1) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.20.1) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.20.1) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.20.1) (1.21.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.20.1) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.20.1) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.20.1) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.20.1) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.20.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.20.1) (2021.10.8)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.19\n",
      "    Uninstalling huggingface-hub-0.0.19:\n",
      "      Successfully uninstalled huggingface-hub-0.0.19\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.11.3\n",
      "    Uninstalling transformers-4.11.3:\n",
      "      Successfully uninstalled transformers-4.11.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 1.14.0 requires huggingface-hub<0.1.0,>=0.0.19, but you have huggingface-hub 0.11.0 which is incompatible.\u001b[0m\n",
      "Successfully installed huggingface-hub-0.11.0 tokenizers-0.12.1 transformers-4.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362cd169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.20.1\n"
     ]
    }
   ],
   "source": [
    "# 트랜스포머 버젼 수정\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35ffff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279992/279992 [00:08<00:00, 32013.13it/s]\n",
      "100%|██████████| 279992/279992 [00:02<00:00, 93668.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기\n",
    "\n",
    "train_df = pd.read_csv('/aiffel/aiffel/aiffelthon/csv/train.tsv', delimiter = '\\t')\n",
    "\n",
    "# drop columns\n",
    "\n",
    "train_df.drop(columns = ['Id', 'Category'], axis = 1, inplace = True)\n",
    "\n",
    "# 데이터 전처리\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (.) 제거\n",
    "    sentence = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+[/ㄱ-ㅎㅏ-ㅣ]', '', sentence) # 여러개 자음과 모음을 삭제한다.\n",
    "    sentence = re.sub(\"[^가-힣a-z0-9-.,!?]\", \" \", sentence) # 지정한 문자 제외 공백으로 전환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "\n",
    "    return sentence\n",
    "\n",
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "clean_text = []\n",
    "\n",
    "for s in tqdm(train_df['Text']):\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "    \n",
    "train_df['Text'] = clean_text\n",
    "\n",
    "# 전체 headlines 데이터에 대한 전처리 \n",
    "clean_headlines = []\n",
    "\n",
    "for s in tqdm(train_df['Summary']):\n",
    "      clean_headlines.append(preprocess_sentence(s))\n",
    "        \n",
    "train_df['Summary'] = clean_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a79376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "\n",
    "dataset = [InputExample(\n",
    "        meta = {'text_a': train_df['Text'][0]},\n",
    "        tgt_text = train_df['Summary'][0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44231c06",
   "metadata": {},
   "source": [
    "# 2단계. PLM 확보\n",
    "\n",
    "- 작업을 지원하는 PLM을 선택하십시오. 모델마다 속성이 다르므로 OpenPrompt를 사용하여 다양한 PLM의 잠재력을 탐색하는 것이 좋습니다. OpenPrompt는 huggingface의 모델과 호환되며 다음 모델이 테스트되었습니다.\n",
    "\n",
    "- 마스킹된 언어 모델(MLM): BERT, RoBERTa,ALBERT\n",
    "- 자기회귀 언어 모델(LM): GPT,GPT2\n",
    "- 시퀀스 대 시퀀스 모델(Seq2Seq):T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3b2d8",
   "metadata": {},
   "source": [
    "### 발생한 오류\n",
    "\n",
    "- ImportError: cannot import name 'OPTConfig' from 'transformers' (/opt/conda/lib/python3.9/site-packages/transformers/__init__.py)\n",
    "- 해결방법 : transformers version == 4.20.1로 수정\n",
    "- link : https://github.com/huggingface/transformers/issues/17790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf9aa3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openprompt에서 사용할 수 있는 Model 및 토크나이저\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer, BertModel, BertForMaskedLM, \\\n",
    "                         RobertaConfig, RobertaTokenizer, RobertaModel, RobertaForMaskedLM, \\\n",
    "                         AlbertTokenizer, AlbertConfig, AlbertModel, AlbertForMaskedLM, \\\n",
    "                         T5Config, T5Tokenizer, T5ForConditionalGeneration, \\\n",
    "                         OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, OpenAIGPTConfig, \\\n",
    "                         GPT2Config, GPT2Tokenizer, GPT2LMHeadModel, \\\n",
    "                         ElectraConfig, ElectraForMaskedLM, ElectraTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edef114",
   "metadata": {},
   "source": [
    "### 기존에 사용하던 Sk-KoGPT-2의 토크나이저가 적용이 되지 않아 kodialogpt2로 바꿔서 적용해보기로 하였다. 해당 Model은 대화문 data를 학습시킨 Model이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deef4539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"gpt2\", \"taeminlee/kodialogpt2-base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048bb84c",
   "metadata": {},
   "source": [
    "# 3단계. 템플릿 정의\n",
    "\n",
    "- A Template는 원본 입력 텍스트의 수정자이며 프롬프트 학습에서 가장 중요한 모듈 중 하나이기도 합니다. 템플릿을 정의하는 고급 자습서는 템플릿 을 작성하는 방법에 있습니다.\n",
    "\n",
    "- 다음은 the가 in <text_a>으로 대체되고 the 가 레이블 단어를 예측하는 데 사용되는 예입니다.text_aInputExample<mask>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8acd9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.prompts import PtuningTemplate \n",
    "promptTemplate = PtuningTemplate(\n",
    "    model = plm,\n",
    "    tokenizer = tokenizer,\n",
    "    prompt_encoder_type = 'lstm',\n",
    "    text = '{\"placeholder\":\"text_a\"}을 요약하자면 {\"mask\":\"text_b\"}',\n",
    "    placeholder_mapping = {'<text_a>': 'text_a', '<text_b>': 'text_b'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d6b13",
   "metadata": {},
   "source": [
    "# 4단계. Verbalizer 정의\n",
    "\n",
    "- A Verbalizer는 프롬프트 학습에서 또 다른 중요하지만(생성에서와 같이 필요하지는 않음) 원래 레이블(우리는 classes레이블 단어 집합으로 정의했습니다. 기억하십니까?)을 투사합니다. Verbalizer 를 정의하는 고급 자습서 는 Verbalizer를 작성하는 방법에 있습니다.\n",
    "\n",
    "- project the negative class to the word bad\n",
    "\n",
    "- project the positive class to the words good, wonderful, great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4add5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.prompts import GenerationVerbalizer \n",
    "promptVerbalizer = GenerationVerbalizer(\n",
    "    tokenizer = tokenizer,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd95a62",
   "metadata": {},
   "source": [
    "# 5단계. PromptModel 구성\n",
    "\n",
    "- 작업이 주어지면 이제 우리는 a PLM, a Template및 a Verbalizer를 가지고 있습니다. 우리는 그것들을 a로 결합합니다 PromptModel.\n",
    "\n",
    "- 이 예제는 순진하게 세 개의 모듈을 결합하지만 실제로 이들 사이에서 몇 가지 복잡한 상호 작용을 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5730111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt import PromptForGeneration\n",
    "promptModel = PromptForGeneration(\n",
    "    template = PtuningTemplate,\n",
    "    plm = plm,\n",
    "    tokenizer = tokenizer,\n",
    "    gen_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4aae51",
   "metadata": {},
   "source": [
    "# 6단계. DataLoader 정의\n",
    "\n",
    "- A 는 기본적으로 a 및 a PromptDataLoader를 포함하는 pytorch Dataloader의 프롬프트 버전입니다 .TokenizerTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea3307a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1it [00:00, 726.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader\n",
    "data_loader = PromptDataLoader(\n",
    "    dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    template = promptTemplate,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88043f2a",
   "metadata": {},
   "source": [
    "# 7단계. 학습 및 추론\n",
    "\n",
    "- 완료! Pytorch에서 다른 프로세스와 동일하게 학습 및 추론을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d79668",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "process_batch() missing 1 required positional argument: 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_243/4009281046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpromptModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aiffel/aiffelthon/prompt/OpenPrompt/openprompt/pipeline_base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputFeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aiffel/aiffelthon/prompt/OpenPrompt/openprompt/pipeline_base.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mreference_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# in case in some template, these field is dropped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift_logits_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aiffel/aiffelthon/prompt/OpenPrompt/openprompt/pipeline_base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputFeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbatchified\u001b[0m \u001b[0mdata\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \"\"\"\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0minput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_keys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: process_batch() missing 1 required positional argument: 'batch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# making zero-shot inference using pretrained MLM with prompt\n",
    "promptModel.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        logits = promptModel(batch)\n",
    "        preds = torch.argmax(logits, dim = -1)\n",
    "        print(tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True))\n",
    "# predictions would be 1, 0 for classes 'positive', 'negative'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
